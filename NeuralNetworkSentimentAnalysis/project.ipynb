{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/natedrake7/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/natedrake7/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics.classification import ConfusionMatrix,F1Score\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./Data/train_set.csv')\n",
    "test_data = pd.read_csv('./Data/test_set.csv')\n",
    "\n",
    "#Create Dataframes\n",
    "train_set = pd.DataFrame(train_data)\n",
    "test_set = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Text and create W2V Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['Tokenized_Text'] = train_set['Text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Create Word2Vec model\n",
    "model = Word2Vec(sentences=train_set['Tokenized_Text'], vector_size=200, window=5, min_count=1, workers=-1)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"./Data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('./Data/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE' 'NEUTRAL' 'POSITIVE']\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "encoder  = LabelEncoder() #initialize label encoder so we can set integer values to the label unique values\n",
    "\n",
    "X_train_labels = encoder.fit_transform(train_set['Sentiment'])\n",
    "\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(encoder.classes_))\n",
    "labels = {0 : 'NEGATIVE',1 : 'NEUTRAL',2 : 'POSITIVE'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_set['Tokenized_Text'], X_train_labels, test_size=0.2 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Word2Vec embeddings to each sentence\n",
    "train_embedded_sentences = [word2vec_model.wv[words] for words in X_train if words]\n",
    "\n",
    "train_sentence_embeddings = [np.mean(embeddings, axis=0) for embeddings in train_embedded_sentences]\n",
    "\n",
    "val_embedded_sentences = [word2vec_model.wv[words] for words in X_val if words]\n",
    "\n",
    "val_sentence_embeddings = [np.mean(embeddings, axis=0) for embeddings in val_embedded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tensorflow tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "#Train set\n",
    "X_train_tensor = torch.tensor(train_sentence_embeddings,dtype=torch.float)\n",
    "Labels_train_tensor = torch.tensor(Y_train,dtype=torch.long) #create a torch tensor for the labels from the numpy array\n",
    "Dataset = TensorDataset(X_train_tensor,Labels_train_tensor) #Create a dataset\n",
    "Train_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True) #Create the train set\n",
    "\n",
    "\n",
    "#Val set\n",
    "X_val_tensor = torch.tensor(val_sentence_embeddings,dtype=torch.float)\n",
    "Labels_val_tensor = torch.tensor(Y_val,dtype=torch.long) #create a torch tensor for the labels from the numpy array\n",
    "Dataset = TensorDataset(X_val_tensor,Labels_val_tensor) #Create a dataset\n",
    "Validation_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True) #Create the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Gpu Or Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): #if nvidia gpu is available\n",
    "   device=\"cuda\" #set devide to cuda since GPUs are much faster at deep learning\n",
    "else: \n",
    "   device=\"cpu\" #else set CPU\n",
    "print(\"Device =\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self): #initialize feedforward network\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential( #Initialize the linear layers\n",
    "            nn.Linear(200,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1), #output is 3 since we have 3 sentiment classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x) #pass the mfccs value through the network\n",
    "        return logits #return the output\n",
    "    \n",
    "model = FeedForwardNeuralNetwork().to(device) #Create a feedforward network instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(dataLoader,model,loss_fn,optimizer,scheduler1 = None,scheduler2 = None):\n",
    "    size = len(dataLoader.dataset) #Get the size of the dataset\n",
    "    for batch, (X,y) in enumerate(dataLoader): #iterate all the dataset\n",
    "\n",
    "        X = X.to(device) #Load variables to GPU\n",
    "        y = y.to(device, dtype=torch.float) #Load variables to GPU\n",
    "        \n",
    "        pred = model(X) #predict the label\n",
    "\n",
    "        y = y.view(-1, 1)\n",
    "\n",
    "        loss = loss_fn(pred,y) #find the loss between the prediction and the true label\n",
    "        #BackPropagation\n",
    "        optimizer.zero_grad() #reset all the gradients\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() #perform a step\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    if scheduler1: #if there is a scheduler available\n",
    "        scheduler1.step() #get the next learning rate\n",
    "    if scheduler2: #if there are chained schedulers\n",
    "        scheduler2.step() #get the next learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(dataloader,model):\n",
    "    size = len(dataloader.dataset) #get the size of the dataset\n",
    "    f1 = 0 #declare f1 score to 0\n",
    "    f1_score = F1Score(task='multiclass',num_classes=3,average='macro').to(device)\n",
    "\n",
    "    with torch.no_grad(): #disable gradient caclulation since we are validating not training\n",
    "        for X,y in dataloader:\n",
    "\n",
    "            X = X.to(device)#Load to GPU\n",
    "            y = y.to(device, dtype=torch.float) #Load variables to GPU\n",
    "        \n",
    "            pred = model(X) #predict the label\n",
    "\n",
    "            y = y.view(-1, 1)\n",
    "            \n",
    "            f1 += f1_score(pred,y) #calculate the f1 score\n",
    "    f1 /= size #find the average f1 score\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(dataloader,model,loss_fn):\n",
    "    size = len(dataloader.dataset) #get the size of the dataset\n",
    "    test_loss,correct,f1 = 0,0,0 #declare variables\n",
    "    f1_score = F1Score(task='multiclass',num_classes=3,average='macro').to(device) \n",
    "\n",
    "    with torch.no_grad(): #disable gradient caclulation since we are testing not training\n",
    "        for X,y in dataloader:\n",
    "\n",
    "            X = X.to(device) #Load to GPU\n",
    "            y = y.to(device, dtype=torch.float) #Load to GPU\n",
    "\n",
    "            pred = model(X) #predict the label\n",
    "\n",
    "            old_y = y\n",
    "            y = y.view(-1, 1)\n",
    "\n",
    "            test_loss += loss_fn(pred,y).item() #find the loss_fn\n",
    "            \n",
    "            correct += (pred.argmax(1) == old_y).type(torch.float).sum().item() #find the accuracy\n",
    "            f1 += f1_score(pred,y) #find the f1 score\n",
    "    \n",
    "    test_loss /= size #compute the average loss\n",
    "    correct /= size #average accuracy\n",
    "    f1 /= size #average f1 score\n",
    "    confmat = ConfusionMatrix('multiclass',num_classes=3).to(device)\n",
    "    confusion_matrix = confmat(pred,y) #find the confusion matrix\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f},Avg F1-Score: {f1:>8f}\\n\") #print statistics\n",
    "    print(f\"Confusion Matrix \\n: {confusion_matrix}\\n\") #print confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 11.994993  [    0/29304]\n",
      "loss: 9.288376  [ 1600/29304]\n",
      "loss: 7.744741  [ 3200/29304]\n",
      "loss: 11.507072  [ 4800/29304]\n",
      "loss: 13.474834  [ 6400/29304]\n",
      "loss: 9.917650  [ 8000/29304]\n",
      "loss: 12.322187  [ 9600/29304]\n",
      "loss: 12.983179  [11200/29304]\n",
      "loss: 13.710409  [12800/29304]\n",
      "loss: 9.998266  [14400/29304]\n",
      "loss: 10.536527  [16000/29304]\n",
      "loss: 10.124269  [17600/29304]\n",
      "loss: 11.076577  [19200/29304]\n",
      "loss: 11.924091  [20800/29304]\n",
      "loss: 8.035734  [22400/29304]\n",
      "loss: 11.134641  [24000/29304]\n",
      "loss: 12.265488  [25600/29304]\n",
      "loss: 9.693432  [27200/29304]\n",
      "loss: 12.145502  [28800/29304]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 11.115925  [    0/29304]\n",
      "loss: 9.012704  [ 1600/29304]\n",
      "loss: 10.167179  [ 3200/29304]\n",
      "loss: 8.542454  [ 4800/29304]\n",
      "loss: 12.006057  [ 6400/29304]\n",
      "loss: 6.932095  [ 8000/29304]\n",
      "loss: 7.849139  [ 9600/29304]\n",
      "loss: 9.414249  [11200/29304]\n",
      "loss: 10.791187  [12800/29304]\n",
      "loss: 12.124752  [14400/29304]\n",
      "loss: 10.437691  [16000/29304]\n",
      "loss: 12.855529  [17600/29304]\n",
      "loss: 11.218008  [19200/29304]\n",
      "loss: 11.775471  [20800/29304]\n",
      "loss: 7.053813  [22400/29304]\n",
      "loss: 12.349541  [24000/29304]\n",
      "loss: 12.031578  [25600/29304]\n",
      "loss: 9.612459  [27200/29304]\n",
      "loss: 11.605616  [28800/29304]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 11.026291  [    0/29304]\n",
      "loss: 9.614970  [ 1600/29304]\n",
      "loss: 8.055117  [ 3200/29304]\n",
      "loss: 11.067162  [ 4800/29304]\n",
      "loss: 11.091537  [ 6400/29304]\n",
      "loss: 11.820671  [ 8000/29304]\n",
      "loss: 11.820332  [ 9600/29304]\n",
      "loss: 8.105438  [11200/29304]\n",
      "loss: 10.589633  [12800/29304]\n",
      "loss: 11.695673  [14400/29304]\n",
      "loss: 9.706130  [16000/29304]\n",
      "loss: 9.693223  [17600/29304]\n",
      "loss: 9.979843  [19200/29304]\n",
      "loss: 14.731572  [20800/29304]\n",
      "loss: 11.771686  [22400/29304]\n",
      "loss: 11.992772  [24000/29304]\n",
      "loss: 12.093983  [25600/29304]\n",
      "loss: 11.611243  [27200/29304]\n",
      "loss: 10.756376  [28800/29304]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 10.797358  [    0/29304]\n",
      "loss: 9.843881  [ 1600/29304]\n",
      "loss: 9.394434  [ 3200/29304]\n",
      "loss: 12.698483  [ 4800/29304]\n",
      "loss: 10.629461  [ 6400/29304]\n",
      "loss: 10.050272  [ 8000/29304]\n",
      "loss: 7.532167  [ 9600/29304]\n",
      "loss: 12.663299  [11200/29304]\n",
      "loss: 6.823330  [12800/29304]\n",
      "loss: 8.504301  [14400/29304]\n",
      "loss: 12.921212  [16000/29304]\n",
      "loss: 10.231835  [17600/29304]\n",
      "loss: 8.734246  [19200/29304]\n",
      "loss: 12.429970  [20800/29304]\n",
      "loss: 9.935179  [22400/29304]\n",
      "loss: 6.869920  [24000/29304]\n",
      "loss: 9.157701  [25600/29304]\n",
      "loss: 12.571182  [27200/29304]\n",
      "loss: 9.275082  [28800/29304]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 11.274052  [    0/29304]\n",
      "loss: 10.664574  [ 1600/29304]\n",
      "loss: 11.656868  [ 3200/29304]\n",
      "loss: 7.657034  [ 4800/29304]\n",
      "loss: 10.855038  [ 6400/29304]\n",
      "loss: 12.481172  [ 8000/29304]\n",
      "loss: 9.887041  [ 9600/29304]\n",
      "loss: 8.573082  [11200/29304]\n",
      "loss: 12.480324  [12800/29304]\n",
      "loss: 12.755209  [14400/29304]\n",
      "loss: 11.902124  [16000/29304]\n",
      "loss: 12.036760  [17600/29304]\n",
      "loss: 7.052866  [19200/29304]\n",
      "loss: 10.315538  [20800/29304]\n",
      "loss: 10.560774  [22400/29304]\n",
      "loss: 7.850072  [24000/29304]\n",
      "loss: 11.053076  [25600/29304]\n",
      "loss: 9.435377  [27200/29304]\n",
      "loss: 9.462662  [28800/29304]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 11.840748  [    0/29304]\n",
      "loss: 10.332580  [ 1600/29304]\n",
      "loss: 13.931364  [ 3200/29304]\n",
      "loss: 9.874044  [ 4800/29304]\n",
      "loss: 11.169696  [ 6400/29304]\n",
      "loss: 12.533466  [ 8000/29304]\n",
      "loss: 9.350610  [ 9600/29304]\n",
      "loss: 12.923779  [11200/29304]\n",
      "loss: 11.676730  [12800/29304]\n",
      "loss: 9.525197  [14400/29304]\n",
      "loss: 12.714209  [16000/29304]\n",
      "loss: 5.817374  [17600/29304]\n",
      "loss: 10.175001  [19200/29304]\n",
      "loss: 11.910628  [20800/29304]\n",
      "loss: 10.535339  [22400/29304]\n",
      "loss: 12.691325  [24000/29304]\n",
      "loss: 11.288370  [25600/29304]\n",
      "loss: 12.020128  [27200/29304]\n",
      "loss: 12.487031  [28800/29304]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 5.862960  [    0/29304]\n",
      "loss: 10.420145  [ 1600/29304]\n",
      "loss: 11.265252  [ 3200/29304]\n",
      "loss: 10.207202  [ 4800/29304]\n",
      "loss: 10.082708  [ 6400/29304]\n",
      "loss: 9.284277  [ 8000/29304]\n",
      "loss: 12.355129  [ 9600/29304]\n",
      "loss: 11.723785  [11200/29304]\n",
      "loss: 8.825668  [12800/29304]\n",
      "loss: 12.819989  [14400/29304]\n",
      "loss: 10.769376  [16000/29304]\n",
      "loss: 10.014559  [17600/29304]\n",
      "loss: 10.912434  [19200/29304]\n",
      "loss: 9.340120  [20800/29304]\n",
      "loss: 9.861269  [22400/29304]\n",
      "loss: 8.839541  [24000/29304]\n",
      "loss: 12.605003  [25600/29304]\n",
      "loss: 10.933642  [27200/29304]\n",
      "loss: 10.015190  [28800/29304]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 12.845479  [    0/29304]\n",
      "loss: 9.851822  [ 1600/29304]\n",
      "loss: 12.585185  [ 3200/29304]\n",
      "loss: 8.369768  [ 4800/29304]\n",
      "loss: 8.972591  [ 6400/29304]\n",
      "loss: 9.900698  [ 8000/29304]\n",
      "loss: 12.938820  [ 9600/29304]\n",
      "loss: 10.666698  [11200/29304]\n",
      "loss: 10.162543  [12800/29304]\n",
      "loss: 10.328437  [14400/29304]\n",
      "loss: 11.823683  [16000/29304]\n",
      "loss: 10.839714  [17600/29304]\n",
      "loss: 9.646544  [19200/29304]\n",
      "loss: 12.953362  [20800/29304]\n",
      "loss: 12.251031  [22400/29304]\n",
      "loss: 10.348621  [24000/29304]\n",
      "loss: 11.471680  [25600/29304]\n",
      "loss: 12.624305  [27200/29304]\n",
      "loss: 10.371663  [28800/29304]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 10.544902  [    0/29304]\n",
      "loss: 12.190325  [ 1600/29304]\n",
      "loss: 8.136263  [ 3200/29304]\n",
      "loss: 9.743207  [ 4800/29304]\n",
      "loss: 11.201038  [ 6400/29304]\n",
      "loss: 9.331054  [ 8000/29304]\n",
      "loss: 9.079170  [ 9600/29304]\n",
      "loss: 11.681555  [11200/29304]\n",
      "loss: 10.389477  [12800/29304]\n",
      "loss: 8.765080  [14400/29304]\n",
      "loss: 11.058794  [16000/29304]\n",
      "loss: 10.948536  [17600/29304]\n",
      "loss: 10.895695  [19200/29304]\n",
      "loss: 10.766817  [20800/29304]\n",
      "loss: 8.983457  [22400/29304]\n",
      "loss: 11.393890  [24000/29304]\n",
      "loss: 10.362150  [25600/29304]\n",
      "loss: 10.905503  [27200/29304]\n",
      "loss: 13.219652  [28800/29304]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 11.353783  [    0/29304]\n",
      "loss: 10.766179  [ 1600/29304]\n",
      "loss: 11.776115  [ 3200/29304]\n",
      "loss: 13.150061  [ 4800/29304]\n",
      "loss: 12.262329  [ 6400/29304]\n",
      "loss: 13.326193  [ 8000/29304]\n",
      "loss: 7.800080  [ 9600/29304]\n",
      "loss: 10.310210  [11200/29304]\n",
      "loss: 11.571409  [12800/29304]\n",
      "loss: 9.872812  [14400/29304]\n",
      "loss: 8.796923  [16000/29304]\n",
      "loss: 10.791649  [17600/29304]\n",
      "loss: 10.845634  [19200/29304]\n",
      "loss: 13.766349  [20800/29304]\n",
      "loss: 12.680358  [22400/29304]\n",
      "loss: 12.320721  [24000/29304]\n",
      "loss: 7.804250  [25600/29304]\n",
      "loss: 13.392151  [27200/29304]\n",
      "loss: 11.975576  [28800/29304]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 10.845443  [    0/29304]\n",
      "loss: 9.741369  [ 1600/29304]\n",
      "loss: 6.686523  [ 3200/29304]\n",
      "loss: 11.634724  [ 4800/29304]\n",
      "loss: 12.179976  [ 6400/29304]\n",
      "loss: 9.583748  [ 8000/29304]\n",
      "loss: 10.320176  [ 9600/29304]\n",
      "loss: 10.872260  [11200/29304]\n",
      "loss: 11.441448  [12800/29304]\n",
      "loss: 9.972479  [14400/29304]\n",
      "loss: 9.920918  [16000/29304]\n",
      "loss: 11.451277  [17600/29304]\n",
      "loss: 9.918686  [19200/29304]\n",
      "loss: 11.595159  [20800/29304]\n",
      "loss: 9.160545  [22400/29304]\n",
      "loss: 12.343287  [24000/29304]\n",
      "loss: 7.217346  [25600/29304]\n",
      "loss: 7.591292  [27200/29304]\n",
      "loss: 10.355009  [28800/29304]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 8.423486  [    0/29304]\n",
      "loss: 9.620249  [ 1600/29304]\n",
      "loss: 8.492586  [ 3200/29304]\n",
      "loss: 10.075802  [ 4800/29304]\n",
      "loss: 13.488118  [ 6400/29304]\n",
      "loss: 8.908152  [ 8000/29304]\n",
      "loss: 11.339078  [ 9600/29304]\n",
      "loss: 7.838198  [11200/29304]\n",
      "loss: 13.433949  [12800/29304]\n",
      "loss: 11.691517  [14400/29304]\n",
      "loss: 9.605258  [16000/29304]\n",
      "loss: 6.752009  [17600/29304]\n",
      "loss: 11.431841  [19200/29304]\n",
      "loss: 10.219158  [20800/29304]\n",
      "loss: 12.851586  [22400/29304]\n",
      "loss: 9.938183  [24000/29304]\n",
      "loss: 11.561501  [25600/29304]\n",
      "loss: 10.347417  [27200/29304]\n",
      "loss: 10.991082  [28800/29304]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 9.397411  [    0/29304]\n",
      "loss: 10.827141  [ 1600/29304]\n",
      "loss: 9.228447  [ 3200/29304]\n",
      "loss: 12.087341  [ 4800/29304]\n",
      "loss: 11.177492  [ 6400/29304]\n",
      "loss: 12.428884  [ 8000/29304]\n",
      "loss: 12.739958  [ 9600/29304]\n",
      "loss: 10.235868  [11200/29304]\n",
      "loss: 7.788061  [12800/29304]\n",
      "loss: 10.111083  [14400/29304]\n",
      "loss: 8.946703  [16000/29304]\n",
      "loss: 12.926348  [17600/29304]\n",
      "loss: 10.903481  [19200/29304]\n",
      "loss: 11.944077  [20800/29304]\n",
      "loss: 12.773493  [22400/29304]\n",
      "loss: 9.727999  [24000/29304]\n",
      "loss: 11.417919  [25600/29304]\n",
      "loss: 7.263525  [27200/29304]\n",
      "loss: 8.503874  [28800/29304]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 11.882321  [    0/29304]\n",
      "loss: 10.915812  [ 1600/29304]\n",
      "loss: 8.792136  [ 3200/29304]\n",
      "loss: 11.277791  [ 4800/29304]\n",
      "loss: 12.451178  [ 6400/29304]\n",
      "loss: 9.112316  [ 8000/29304]\n",
      "loss: 10.897396  [ 9600/29304]\n",
      "loss: 11.251914  [11200/29304]\n",
      "loss: 12.437364  [12800/29304]\n",
      "loss: 7.396577  [14400/29304]\n",
      "loss: 12.691894  [16000/29304]\n",
      "loss: 11.001862  [17600/29304]\n",
      "loss: 11.594799  [19200/29304]\n",
      "loss: 11.058973  [20800/29304]\n",
      "loss: 9.994295  [22400/29304]\n",
      "loss: 8.552929  [24000/29304]\n",
      "loss: 10.449238  [25600/29304]\n",
      "loss: 9.192042  [27200/29304]\n",
      "loss: 12.639150  [28800/29304]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 10.090424  [    0/29304]\n",
      "loss: 10.745863  [ 1600/29304]\n",
      "loss: 14.145411  [ 3200/29304]\n",
      "loss: 12.099113  [ 4800/29304]\n",
      "loss: 9.032383  [ 6400/29304]\n",
      "loss: 8.909282  [ 8000/29304]\n",
      "loss: 11.048597  [ 9600/29304]\n",
      "loss: 13.097200  [11200/29304]\n",
      "loss: 10.399027  [12800/29304]\n",
      "loss: 10.380486  [14400/29304]\n",
      "loss: 9.489870  [16000/29304]\n",
      "loss: 10.356040  [17600/29304]\n",
      "loss: 8.243667  [19200/29304]\n",
      "loss: 11.079069  [20800/29304]\n",
      "loss: 13.226306  [22400/29304]\n",
      "loss: 11.445894  [24000/29304]\n",
      "loss: 10.427519  [25600/29304]\n",
      "loss: 9.069855  [27200/29304]\n",
      "loss: 9.185304  [28800/29304]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 9.017619  [    0/29304]\n",
      "loss: 10.361260  [ 1600/29304]\n",
      "loss: 9.046165  [ 3200/29304]\n",
      "loss: 7.512685  [ 4800/29304]\n",
      "loss: 11.490088  [ 6400/29304]\n",
      "loss: 9.680124  [ 8000/29304]\n",
      "loss: 12.684638  [ 9600/29304]\n",
      "loss: 12.351396  [11200/29304]\n",
      "loss: 9.687798  [12800/29304]\n",
      "loss: 11.592201  [14400/29304]\n",
      "loss: 12.814674  [16000/29304]\n",
      "loss: 11.978746  [17600/29304]\n",
      "loss: 10.895461  [19200/29304]\n",
      "loss: 11.513153  [20800/29304]\n",
      "loss: 11.904025  [22400/29304]\n",
      "loss: 7.034142  [24000/29304]\n",
      "loss: 9.669855  [25600/29304]\n",
      "loss: 8.024137  [27200/29304]\n",
      "loss: 10.407629  [28800/29304]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 11.834669  [    0/29304]\n",
      "loss: 10.605221  [ 1600/29304]\n",
      "loss: 9.884003  [ 3200/29304]\n",
      "loss: 8.999991  [ 4800/29304]\n",
      "loss: 15.455915  [ 6400/29304]\n",
      "loss: 12.283349  [ 8000/29304]\n",
      "loss: 13.026283  [ 9600/29304]\n",
      "loss: 11.285435  [11200/29304]\n",
      "loss: 8.708242  [12800/29304]\n",
      "loss: 10.298680  [14400/29304]\n",
      "loss: 12.986114  [16000/29304]\n",
      "loss: 12.257967  [17600/29304]\n",
      "loss: 10.722916  [19200/29304]\n",
      "loss: 11.208101  [20800/29304]\n",
      "loss: 6.240406  [22400/29304]\n",
      "loss: 10.228731  [24000/29304]\n",
      "loss: 12.929878  [25600/29304]\n",
      "loss: 10.569297  [27200/29304]\n",
      "loss: 11.473227  [28800/29304]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 10.069605  [    0/29304]\n",
      "loss: 12.404802  [ 1600/29304]\n",
      "loss: 10.785782  [ 3200/29304]\n",
      "loss: 12.223402  [ 4800/29304]\n",
      "loss: 14.105862  [ 6400/29304]\n",
      "loss: 11.638089  [ 8000/29304]\n",
      "loss: 10.516163  [ 9600/29304]\n",
      "loss: 10.548538  [11200/29304]\n",
      "loss: 10.309117  [12800/29304]\n",
      "loss: 10.186425  [14400/29304]\n",
      "loss: 11.341137  [16000/29304]\n",
      "loss: 8.457602  [17600/29304]\n",
      "loss: 10.751384  [19200/29304]\n",
      "loss: 11.086475  [20800/29304]\n",
      "loss: 8.314230  [22400/29304]\n",
      "loss: 10.691380  [24000/29304]\n",
      "loss: 12.159147  [25600/29304]\n",
      "loss: 11.885268  [27200/29304]\n",
      "loss: 14.923564  [28800/29304]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 13.064732  [    0/29304]\n",
      "loss: 9.875664  [ 1600/29304]\n",
      "loss: 12.160354  [ 3200/29304]\n",
      "loss: 13.037699  [ 4800/29304]\n",
      "loss: 11.295298  [ 6400/29304]\n",
      "loss: 9.182395  [ 8000/29304]\n",
      "loss: 13.749780  [ 9600/29304]\n",
      "loss: 11.468493  [11200/29304]\n",
      "loss: 12.098726  [12800/29304]\n",
      "loss: 11.365860  [14400/29304]\n",
      "loss: 12.964612  [16000/29304]\n",
      "loss: 11.002703  [17600/29304]\n",
      "loss: 8.241500  [19200/29304]\n",
      "loss: 12.347042  [20800/29304]\n",
      "loss: 10.778446  [22400/29304]\n",
      "loss: 8.742124  [24000/29304]\n",
      "loss: 11.637331  [25600/29304]\n",
      "loss: 8.352831  [27200/29304]\n",
      "loss: 10.390220  [28800/29304]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 11.095956  [    0/29304]\n",
      "loss: 9.894316  [ 1600/29304]\n",
      "loss: 11.325212  [ 3200/29304]\n",
      "loss: 12.166872  [ 4800/29304]\n",
      "loss: 10.989360  [ 6400/29304]\n",
      "loss: 13.434038  [ 8000/29304]\n",
      "loss: 9.040997  [ 9600/29304]\n",
      "loss: 10.175235  [11200/29304]\n",
      "loss: 7.815385  [12800/29304]\n",
      "loss: 10.541809  [14400/29304]\n",
      "loss: 10.713245  [16000/29304]\n",
      "loss: 8.052017  [17600/29304]\n",
      "loss: 10.952669  [19200/29304]\n",
      "loss: 12.115395  [20800/29304]\n",
      "loss: 9.208150  [22400/29304]\n",
      "loss: 8.593759  [24000/29304]\n",
      "loss: 8.830581  [25600/29304]\n",
      "loss: 10.872946  [27200/29304]\n",
      "loss: 7.591264  [28800/29304]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 12.442923  [    0/29304]\n",
      "loss: 13.526800  [ 1600/29304]\n",
      "loss: 11.997700  [ 3200/29304]\n",
      "loss: 12.095695  [ 4800/29304]\n",
      "loss: 13.277067  [ 6400/29304]\n",
      "loss: 14.292133  [ 8000/29304]\n",
      "loss: 11.628155  [ 9600/29304]\n",
      "loss: 9.803041  [11200/29304]\n",
      "loss: 12.302336  [12800/29304]\n",
      "loss: 7.960817  [14400/29304]\n",
      "loss: 11.351175  [16000/29304]\n",
      "loss: 10.463032  [17600/29304]\n",
      "loss: 10.097347  [19200/29304]\n",
      "loss: 8.885022  [20800/29304]\n",
      "loss: 11.257073  [22400/29304]\n",
      "loss: 13.409275  [24000/29304]\n",
      "loss: 9.713401  [25600/29304]\n",
      "loss: 10.469426  [27200/29304]\n",
      "loss: 8.566759  [28800/29304]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 9.490713  [    0/29304]\n",
      "loss: 14.298118  [ 1600/29304]\n",
      "loss: 13.609312  [ 3200/29304]\n",
      "loss: 12.417305  [ 4800/29304]\n",
      "loss: 9.792931  [ 6400/29304]\n",
      "loss: 9.538261  [ 8000/29304]\n",
      "loss: 9.172276  [ 9600/29304]\n",
      "loss: 11.807524  [11200/29304]\n",
      "loss: 9.313033  [12800/29304]\n",
      "loss: 12.613447  [14400/29304]\n",
      "loss: 10.122252  [16000/29304]\n",
      "loss: 10.699423  [17600/29304]\n",
      "loss: 11.112419  [19200/29304]\n",
      "loss: 8.193842  [20800/29304]\n",
      "loss: 10.268539  [22400/29304]\n",
      "loss: 8.491770  [24000/29304]\n",
      "loss: 10.838058  [25600/29304]\n",
      "loss: 8.225060  [27200/29304]\n",
      "loss: 9.548960  [28800/29304]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 9.410929  [    0/29304]\n",
      "loss: 9.259011  [ 1600/29304]\n",
      "loss: 12.500059  [ 3200/29304]\n",
      "loss: 13.304634  [ 4800/29304]\n",
      "loss: 11.484503  [ 6400/29304]\n",
      "loss: 11.098593  [ 8000/29304]\n",
      "loss: 8.109699  [ 9600/29304]\n",
      "loss: 11.581884  [11200/29304]\n",
      "loss: 11.260849  [12800/29304]\n",
      "loss: 13.899145  [14400/29304]\n",
      "loss: 12.894740  [16000/29304]\n",
      "loss: 10.257656  [17600/29304]\n",
      "loss: 13.021071  [19200/29304]\n",
      "loss: 10.143947  [20800/29304]\n",
      "loss: 11.666107  [22400/29304]\n",
      "loss: 11.950581  [24000/29304]\n",
      "loss: 11.594194  [25600/29304]\n",
      "loss: 8.875824  [27200/29304]\n",
      "loss: 10.974562  [28800/29304]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 7.907908  [    0/29304]\n",
      "loss: 11.606840  [ 1600/29304]\n",
      "loss: 12.402775  [ 3200/29304]\n",
      "loss: 8.663440  [ 4800/29304]\n",
      "loss: 12.295476  [ 6400/29304]\n",
      "loss: 9.623207  [ 8000/29304]\n",
      "loss: 9.298923  [ 9600/29304]\n",
      "loss: 9.363848  [11200/29304]\n",
      "loss: 13.806394  [12800/29304]\n",
      "loss: 10.903766  [14400/29304]\n",
      "loss: 11.528778  [16000/29304]\n",
      "loss: 10.130070  [17600/29304]\n",
      "loss: 5.491682  [19200/29304]\n",
      "loss: 11.470829  [20800/29304]\n",
      "loss: 9.848019  [22400/29304]\n",
      "loss: 11.524029  [24000/29304]\n",
      "loss: 7.714783  [25600/29304]\n",
      "loss: 9.880362  [27200/29304]\n",
      "loss: 10.844355  [28800/29304]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 10.661060  [    0/29304]\n",
      "loss: 11.744663  [ 1600/29304]\n",
      "loss: 11.735794  [ 3200/29304]\n",
      "loss: 11.365726  [ 4800/29304]\n",
      "loss: 13.531286  [ 6400/29304]\n",
      "loss: 11.296818  [ 8000/29304]\n",
      "loss: 10.299702  [ 9600/29304]\n",
      "loss: 10.516512  [11200/29304]\n",
      "loss: 11.218232  [12800/29304]\n",
      "loss: 13.317498  [14400/29304]\n",
      "loss: 8.947650  [16000/29304]\n",
      "loss: 13.996241  [17600/29304]\n",
      "loss: 10.724957  [19200/29304]\n",
      "loss: 10.852156  [20800/29304]\n",
      "loss: 9.339545  [22400/29304]\n",
      "loss: 8.977461  [24000/29304]\n",
      "loss: 10.053539  [25600/29304]\n",
      "loss: 12.576643  [27200/29304]\n",
      "loss: 10.261797  [28800/29304]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 11.033231  [    0/29304]\n",
      "loss: 9.491617  [ 1600/29304]\n",
      "loss: 11.773266  [ 3200/29304]\n",
      "loss: 10.880450  [ 4800/29304]\n",
      "loss: 10.938543  [ 6400/29304]\n",
      "loss: 9.091347  [ 8000/29304]\n",
      "loss: 10.055326  [ 9600/29304]\n",
      "loss: 12.127291  [11200/29304]\n",
      "loss: 7.916032  [12800/29304]\n",
      "loss: 9.781370  [14400/29304]\n",
      "loss: 14.252657  [16000/29304]\n",
      "loss: 10.345085  [17600/29304]\n",
      "loss: 9.792919  [19200/29304]\n",
      "loss: 14.106826  [20800/29304]\n",
      "loss: 10.030085  [22400/29304]\n",
      "loss: 9.020252  [24000/29304]\n",
      "loss: 10.665836  [25600/29304]\n",
      "loss: 11.636671  [27200/29304]\n",
      "loss: 7.692513  [28800/29304]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 7.175647  [    0/29304]\n",
      "loss: 14.064546  [ 1600/29304]\n",
      "loss: 10.388950  [ 3200/29304]\n",
      "loss: 9.935152  [ 4800/29304]\n",
      "loss: 12.548749  [ 6400/29304]\n",
      "loss: 13.037671  [ 8000/29304]\n",
      "loss: 10.170275  [ 9600/29304]\n",
      "loss: 8.032635  [11200/29304]\n",
      "loss: 8.709721  [12800/29304]\n",
      "loss: 9.482733  [14400/29304]\n",
      "loss: 12.030279  [16000/29304]\n",
      "loss: 9.255926  [17600/29304]\n",
      "loss: 8.285679  [19200/29304]\n",
      "loss: 11.905319  [20800/29304]\n",
      "loss: 12.680052  [22400/29304]\n",
      "loss: 11.430955  [24000/29304]\n",
      "loss: 10.300076  [25600/29304]\n",
      "loss: 8.698697  [27200/29304]\n",
      "loss: 8.239363  [28800/29304]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 9.663292  [    0/29304]\n",
      "loss: 6.882975  [ 1600/29304]\n",
      "loss: 10.097629  [ 3200/29304]\n",
      "loss: 10.132253  [ 4800/29304]\n",
      "loss: 11.922957  [ 6400/29304]\n",
      "loss: 10.812195  [ 8000/29304]\n",
      "loss: 8.974680  [ 9600/29304]\n",
      "loss: 9.798439  [11200/29304]\n",
      "loss: 13.772911  [12800/29304]\n",
      "loss: 6.505606  [14400/29304]\n",
      "loss: 7.646743  [16000/29304]\n",
      "loss: 7.838693  [17600/29304]\n",
      "loss: 9.700665  [19200/29304]\n",
      "loss: 9.185984  [20800/29304]\n",
      "loss: 10.665380  [22400/29304]\n",
      "loss: 7.633194  [24000/29304]\n",
      "loss: 8.816451  [25600/29304]\n",
      "loss: 6.652611  [27200/29304]\n",
      "loss: 7.928751  [28800/29304]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 10.137755  [    0/29304]\n",
      "loss: 10.221115  [ 1600/29304]\n",
      "loss: 8.834472  [ 3200/29304]\n",
      "loss: 9.478560  [ 4800/29304]\n",
      "loss: 9.249512  [ 6400/29304]\n",
      "loss: 9.852690  [ 8000/29304]\n",
      "loss: 12.000967  [ 9600/29304]\n",
      "loss: 7.969627  [11200/29304]\n",
      "loss: 10.111221  [12800/29304]\n",
      "loss: 11.429811  [14400/29304]\n",
      "loss: 10.584965  [16000/29304]\n",
      "loss: 9.193765  [17600/29304]\n",
      "loss: 9.741338  [19200/29304]\n",
      "loss: 8.517725  [20800/29304]\n",
      "loss: 15.712168  [22400/29304]\n",
      "loss: 8.327778  [24000/29304]\n",
      "loss: 9.231375  [25600/29304]\n",
      "loss: 8.541704  [27200/29304]\n",
      "loss: 8.215903  [28800/29304]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 8.743771  [    0/29304]\n",
      "loss: 9.379389  [ 1600/29304]\n",
      "loss: 10.440037  [ 3200/29304]\n",
      "loss: 13.354549  [ 4800/29304]\n",
      "loss: 9.017487  [ 6400/29304]\n",
      "loss: 12.339030  [ 8000/29304]\n",
      "loss: 9.414503  [ 9600/29304]\n",
      "loss: 10.174572  [11200/29304]\n",
      "loss: 15.086344  [12800/29304]\n",
      "loss: 10.961817  [14400/29304]\n",
      "loss: 13.753634  [16000/29304]\n",
      "loss: 12.565001  [17600/29304]\n",
      "loss: 13.677120  [19200/29304]\n",
      "loss: 11.606419  [20800/29304]\n",
      "loss: 10.280876  [22400/29304]\n",
      "loss: 13.426950  [24000/29304]\n",
      "loss: 12.841412  [25600/29304]\n",
      "loss: 11.066459  [27200/29304]\n",
      "loss: 9.279011  [28800/29304]\n",
      "2447.0\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 0.660774,Avg F1-Score: 0.016823\n",
      "\n",
      "Confusion Matrix \n",
      ": tensor([[1, 2, 0],\n",
      "        [5, 1, 0],\n",
      "        [3, 2, 0]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "best_f1 = 0 #initialize variables\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) #set optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate) #set optimizer\n",
    "\n",
    "epochs_count = 30 #set epochs count\n",
    "\n",
    "for t in range(epochs_count): #iterate over the entire dataset for the number of epochs specified\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    Train(Train_set,model,loss_fn,optimizer) #train the model\n",
    "    f1 = Validation(Validation_set,model) #find the f1 score for the validation set\n",
    "    if f1 >= best_f1: #if the f1 score of the validation set is the best one\n",
    "        best_f1 = f1 #set it to the best f1 variable\n",
    "        best_model = model #set best model equal to the current model\n",
    "#print(f\"Best F1 Score: {best_f1:>8f}\\n\")\n",
    "Test(Validation_set,best_model,loss_fn)#test based on the best model found by the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.010197\n",
    "Best F1 Score: 0.010684\n",
    "\n",
    "Best F1 Score: 0.017475 AdamW\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
