{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project implemented by Konstantinos Lampropoulos $$ $$\n",
    "AM:1115201800092"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Math,Latex\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics.classification import ConfusionMatrix,F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 : FeedForward Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train set\n",
    "X_train = np.load('./music_genre_data_di/train/mfccs/X.npy')\n",
    "Labels_train = np.load('./music_genre_data_di/train/mfccs/labels.npy')\n",
    "\n",
    "#Load Validation set\n",
    "X_val = np.load('./music_genre_data_di/val/mfccs/X.npy')\n",
    "Labels_val = np.load('./music_genre_data_di/val/mfccs/labels.npy')\n",
    "\n",
    "#Load Test set\n",
    "X_test = np.load('./music_genre_data_di/test/mfccs/X.npy')\n",
    "Labels_test = np.load('./music_genre_data_di/test/mfccs/labels.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = LabelEncoder()\n",
    "\n",
    "Labels_train_encoded = encoder.fit_transform(Labels_train)\n",
    "\n",
    "Labels_val_encoded = encoder.fit_transform(Labels_val)\n",
    "\n",
    "Labels_test_encoded = encoder.fit_transform(Labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create torch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#Train set\n",
    "X_train_tensor = torch.tensor(X_train,dtype=torch.float32)\n",
    "Labels_train_tensor = torch.tensor(Labels_train_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_train_tensor,Labels_train_tensor)\n",
    "Train_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#Val set\n",
    "X_val_tensor = torch.tensor(X_val,dtype=torch.float32)\n",
    "Labels_val_tensor = torch.tensor(Labels_val_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_val_tensor,Labels_val_tensor)\n",
    "Validation_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#Test set\n",
    "X_test_tensor = torch.tensor(X_test,dtype=torch.float32)\n",
    "Labels_test_tensor = torch.tensor(Labels_test_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_test_tensor,Labels_test_tensor)\n",
    "Test_set = DataLoader(Dataset,batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device=\"cuda\"\n",
    "else:\n",
    "   device=\"cpu\"\n",
    "print(\"Device =\",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feed Forward Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(26,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = FeedForwardNeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(dataLoader,model,loss_fn,optimizer,scheduler = None):\n",
    "    size = len(dataLoader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        #Load to GPU\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        #Prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        #BackPropagation\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    if scheduler:\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(dataloader,model,loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss,correct,f1 = 0,0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            #Load to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            f1_score = F1Score(task='multiclass',num_classes=4,average='macro').to(device)\n",
    "            f1 += f1_score(pred.argmax(1),y)\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    f1 /= size\n",
    "    confmat = ConfusionMatrix('multiclass',num_classes=4).to(device)\n",
    "    confusion_matrix = confmat(pred.argmax(1),y)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f},Avg F1-Score: {f1:>8f}\\n\")\n",
    "    print(f\"Confusion Matrix \\n: {confusion_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(dataloader,model):\n",
    "    size = len(dataloader.dataset)\n",
    "    f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            #Load to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            f1_score = F1Score(task='multiclass',num_classes=4,average='macro').to(device)\n",
    "            f1 += f1_score(pred.argmax(1),y)\n",
    "    f1 /= size\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_f1 = 0\n",
    "learning_rate = 0.002\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "epochs_count = 30\n",
    "for t in range(epochs_count):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    Train(Train_set,model,loss_fn,optimizer)\n",
    "    f1 = Validation(Validation_set,model)\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "    Test(Test_set,model,loss_fn)\n",
    "print(\"Best Model computed by finding the model with the highest f1 score on the validation set\\n\")\n",
    "Test(Test_set,best_model,loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU vs CPU runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Με την χρήση CPU για την αξιολόγηση του μοντέλου πετυχαίνουμε 60.6% accuracy,Loss 0.060977 και f1  0.011394 σε 6.5s\n",
    "* Με την χρήση GPU για την αξιολόγιση του μοντέλου πετυχαίνουμε 62.4% accuracy,Loss 0.061011 και f1  0.011620 σε 22.4s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 : Convolutional Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "import torch.backends.cudnn\n",
    "\n",
    "SEED = 12345\n",
    "rs = RandomState(MT19937(SeedSequence(SEED)))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "g_cuda = torch.Generator(device=device)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train set\n",
    "X_train = np.load('./music_genre_data_di/train/melgrams/X.npy')\n",
    "Labels_train = np.load('./music_genre_data_di/train/melgrams/labels.npy')\n",
    "\n",
    "#Load Validation set\n",
    "X_val = np.load('./music_genre_data_di/val/melgrams/X.npy')\n",
    "Labels_val = np.load('./music_genre_data_di/val/melgrams/labels.npy')\n",
    "\n",
    "#Load Test set\n",
    "X_test = np.load('./music_genre_data_di/test/melgrams/X.npy')\n",
    "Labels_test = np.load('./music_genre_data_di/test/melgrams/labels.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = LabelEncoder()\n",
    "\n",
    "Labels_train_encoded = encoder.fit_transform(Labels_train)\n",
    "\n",
    "Labels_val_encoded = encoder.fit_transform(Labels_val)\n",
    "\n",
    "Labels_test_encoded = encoder.fit_transform(Labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Torch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "#Train set\n",
    "X_train_tensor = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
    "Labels_train_tensor = torch.tensor(Labels_train_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_train_tensor,Labels_train_tensor)\n",
    "Train_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)\n",
    "\n",
    "#Val set\n",
    "X_val_tensor = torch.tensor(X_val,dtype=torch.float32)\n",
    "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
    "Labels_val_tensor = torch.tensor(Labels_val_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_val_tensor,Labels_val_tensor)\n",
    "Validation_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)\n",
    "\n",
    "#Test set\n",
    "X_test_tensor = torch.tensor(X_test,dtype=torch.float32)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "Labels_test_tensor = torch.tensor(Labels_test_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_test_tensor,Labels_test_tensor)\n",
    "Test_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Convolutional Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self,dropout_rate=0.5):\n",
    "        super(ConvolutionalNeuralNetwork,self).__init__()\n",
    "        kernel_size,padding = 2,1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2,padding=padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=kernel_size)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2,padding=padding)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=kernel_size)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.maxpool3= nn.MaxPool2d(kernel_size=2,padding=padding)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=kernel_size)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(3840, 1024)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(32, 4)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = nn.ELU()(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = nn.ELU()(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool2(x)\n",
    "        x = nn.ELU()(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool3(x)      \n",
    "        x = nn.ELU()(self.bn4(self.conv4(x)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x,1)\n",
    "        # Fully connected layers\n",
    "        x = nn.ELU()(self.fc1(x))\n",
    "        x = nn.ELU()(self.fc2(x))\n",
    "        x = nn.ELU()(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "model = ConvolutionalNeuralNetwork(dropout_rate=0.5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_f1 = 0\n",
    "learning_rate = 0.002\n",
    "patience = 0\n",
    "patience_tol = epochs_count\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,verbose=True,T_max=30)\n",
    "\n",
    "epochs_count = 60\n",
    "for t in range(epochs_count):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    torch.cuda.synchronize()\n",
    "    Train(Train_set,model,loss_fn,optimizer,scheduler)\n",
    "    f1 = Validation(Validation_set,model)\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "    else:\n",
    "        patience+=1\n",
    "    if patience == patience_tol:\n",
    "        break\n",
    "    #Test(Test_set,model,loss_fn)\n",
    "print(\"Best Model computed by finding the model with the highest f1 score on the validation set\\n\")\n",
    "Test(Test_set,best_model,loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU vs CPU runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Στο GPU έτρεξε σε 1m1.6s και επέστρεψε βέλτιστο μοντέλο με accuracy 65.6% ,Avg loss: 0.132755,Avg F1-Score: 0.012216\n",
    "* Στο CPU έτρεξε σε χρόνο 26m25.1s και επέστρεψε βέλτιστο μοντέλο με  Accuracy: 65.0%, Avg loss: 0.152512,Avg F1-Score: 0.012084"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling And Padding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Με max pooling με kernel = 2,padding = 1(αφού το padding πρέπει να είναι το μισό του kernel size) η επίδοση αυξήθηκε στο 70.9% σε χρόνο 40.7s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Optimizer} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{SGD} & \\text{70.9\\%} & \\text{0.012768}\\\\\n",
    "\\hline\n",
    "\\text{Adadelta} & \\text{74.5\\%} & \\text{0.013245}\\\\\n",
    "\\hline\n",
    "\\text{Adagrad} & \\text{75.1\\%} & \\text{0.013355}\\\\\n",
    "\\hline\n",
    "\\text{Adam} & \\text{70.7\\%} & \\text{0.013004}\\\\\n",
    "\\hline\n",
    "\\text{AdamW} & \\text{69.5\\%} & \\text{0.012643}\\\\\n",
    "\\hline\n",
    "\\text{Adamax} & \\text{72.2\\%} & \\text{0.013016}\\\\\n",
    "\\hline\n",
    "\\text{ASGD} & \\text{72.2\\%} & \\text{0.013016}\\\\\n",
    "\\hline\n",
    "\\text{NAdam} & \\text{68.3\\%} & \\text{0.012598}\\\\\n",
    "\\hline\n",
    "\\text{RAdam} & \\text{69.0\\%} & \\text{0.012522}\\\\\n",
    "\\hline\n",
    "\\text{RMsprop} & \\text{70.9\\%} & \\text{0.012781}\\\\\n",
    "\\hline\n",
    "\\text{Rprop} & \\text{71.3\\%} & \\text{0.012988}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers Comparsion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Την καλύτερη απόδοση την είχε ο αλγόριθμος Adagrad που πέτυχε accuracy 75.1% με F1-Score 0.013355.Γενικά η απόκλιση των optimizers είναι μικρή με το μικρότερο accuracy να είναι του NAdam με 68.3% και το μέγιστο του AdaGrad με 75.1%.Οι αποκλίσεις αυτές εξαρτώνται και από το shuffling που υπαρχει στα train και validation sets αλλά και στο ότι κάποιοι αλγόριθμοι αποδίδουν καλύτερα σε διαφορετικά προβλήματα από κάποιους άλλους."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 : Improving Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Activator} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{ReLu} & \\text{32.0\\%} & \\text{0.005272}\\\\\n",
    "\\hline\n",
    "\\text{ELU} & \\text{38.9\\%} & \\text{0.006511}\\\\\n",
    "\\hline\n",
    "\\text{HardShrink} & \\text{33.1\\%} & \\text{0.005456}\\\\\n",
    "\\hline\n",
    "\\text{HardSigmoid} & \\text{23.5\\%} & \\text{0.003706}\\\\\n",
    "\\hline\n",
    "\\text{HardTanh} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{RReLU} & \\text{30.8\\%} & \\text{0.005007}\\\\\n",
    "\\hline\n",
    "\\text{GELU} & \\text{32.3\\%} & \\text{0.005344}\\\\\n",
    "\\hline\n",
    "\\text{SoftPlus} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Scheduler} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{LinearLR} & \\text{30.5\\%} & \\text{0.004912}\\\\\n",
    "\\hline\n",
    "\\text{PolynomialLR} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{ExponentialLR} & \\text{63.7\\%} & \\text{0.011358}\\\\\n",
    "\\hline\n",
    "\\text{MultiStepLR} & \\text{38.9\\%} & \\text{0.006511}\\\\\n",
    "\\hline\n",
    "\\text{CosineAnnealingLR} & \\text{63.3\\%} & \\text{0.011417}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Accuracy: 73.3%, Avg loss: 0.062259,Avg F1-Score: 0.012951"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight Decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Weight Decay} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{0.5} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{1e-2} & \\text{68.5\\%} & \\text{0.011750}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{1e-5} & \\text{75.4\\%} & \\text{0.013106}\\\\\n",
    "\\hline\n",
    "\\text{1e-7} & \\text{75.4\\%} & \\text{.013105}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Dropout Rate} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{0.5} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{0.2} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{0.1} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight Decay and Dropout Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Weight Decay} & \\text{Dropout Rate} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.5} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.2} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Batch Size} & \\text{Accuracy} & \\text{F1-Score} & \\text{Time}\\\\\n",
    "\\hline\n",
    "\\text{2} & \\text{62.1\\%} & \\text{0.083757} & \\text{10m25.6s}\\\\\n",
    "\\hline\n",
    "\\text{4} & \\text{67.4\\%} & \\text{0.046776} & \\text{5m20.2s}\\\\\n",
    "\\hline\n",
    "\\text{8} & \\text{71.9\\%} & \\text{0.025033} & \\text{2m38.7s}\\\\\n",
    "\\hline\n",
    "\\text{16} & \\text{75.4\\%} & \\text{0.013121} & \\text{1m17.2s}\\\\\n",
    "\\hline\n",
    "\\text{32} & \\text{76.6\\%} & \\text{0.006810} & \\text{41.2s}\\\\\n",
    "\\hline\n",
    "\\text{64} & \\text{78.0\\%} & \\text{0.003556} & \\text{21.3s}\\\\\n",
    "\\hline\n",
    "\\text{128} & \\text{76.7\\%} & \\text{0.002045} & \\text{11.9s}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Παρατηρούμε ότι όσο αυξάνεται το batch size αυξάνεται(μέχρι το 64) και το accuracy,αλλά ταυτόχρονα μειώνεται το f1 score.Παρατηρούμε ότι για batch size 2 το f1 score είναι το μέγιστο αλλά χρειαζόμαστε χρόνο 10m25.6s.Αν έπρεπε να επιλέξουμε batch size μόνο με βάση την μετρική f1 ,θα επιλέγαμε το batch size 2.Όμως παίρνοντας ως παράγοντα και το accuracy και τον χρόνο το βέλιστο,το batch size 16 αποτελεί βέλτιστη επιλογή μιάς και έχει accuracy πολύ κοντινό σε αυτό των υπολοίπων,χαμηλό χρόνο εκπαίδευσης και καλύτερο f1 score από αυτό των batch sizes με καλύτερο accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Patience} & \\text{Accuracy} & \\text{F1-Score} & \\text{Time}\\\\\n",
    "\\hline\n",
    "\\text{3} & \\text{73.2\\%} & \\text{0.012941} & \\text{43.3s}\\\\\n",
    "\\hline\n",
    "\\text{5} & \\text{73.1\\%} & \\text{0.012935} & \\text{44.9s}\\\\\n",
    "\\hline\n",
    "\\text{6} & \\text{73.8\\%} & \\text{0.012984} & \\text{49.3s}\\\\\n",
    "\\hline\n",
    "\\text{7} & \\text{74.2\\%} & \\text{0.013015} & \\text{49.8s}\\\\\n",
    "\\hline\n",
    "\\text{10} & \\text{72.7\\%} & \\text{0.012781} & \\text{54.5s}\\\\\n",
    "\\hline\n",
    "\\text{15} & \\text{72.8\\%} & \\text{0.012746} & \\text{1m2.6s}\\\\\n",
    "\\hline\n",
    "\\text{None} & \\text{75.4\\%} & \\text{0.013121} & \\text{1m20.5s}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Παρατηρώντας τα δεδομένα,αν θέλουμε να θέσουμε ένα patience για το δεδομένο μοντέλο,το βέλτιστο θα ήταν 7.Όμως αφού ο χρόνος ήδη δεν είναι πολύ απαιτητικός θα μπορούσαμε να μην θέσουμε."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_genre_data_di.youtube import youtube_to_melgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_Type(type):\n",
    "    if type == 0 : type_str = 'blues'\n",
    "    elif type == 1 : type_str = 'classical'\n",
    "    elif type == 2 : type_str = 'hiphop'\n",
    "    else : type_str = 'rock_metal_hardrock'\n",
    "    return type_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestYT(dataloader,model):\n",
    "    music_type = []\n",
    "    music_percentage = {}\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            music_type.append(pred.argmax(1))\n",
    "    for tensor in music_type:\n",
    "        unique_types, counts = torch.unique(tensor, return_counts=True)\n",
    "        for type, count in zip(unique_types, counts):\n",
    "            if type.item() in music_percentage:\n",
    "                music_percentage[type.item()] += count.item()\n",
    "            else:\n",
    "                music_percentage[type.item()] = count.item()\n",
    "    total_count = sum(music_percentage.values())\n",
    "    percentages = {type: count / total_count * 100 for type, count in music_percentage.items()}              \n",
    "    for type, percentage in percentages.items():\n",
    "        print(f'Type: {Map_Type(type)}, Percentage: {percentage:.2f}%')\n",
    "    return music_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_to_melgram(url = 'https://www.youtube.com/watch?v=9E6b3swbnWg',filename=\"./Classical.wav\")\n",
    "youtube_to_melgram(url = \"https://www.youtube.com/watch?v=vyI9flHHT2Q\",filename=\"./Blues.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./music_genre_data_di/Classical.npy')\n",
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = torch.full((X_tensor.size(0),),0,dtype=torch.long)\n",
    "tensor_dataset = TensorDataset(X_tensor,y_tensor)\n",
    "data = DataLoader(tensor_dataset,shuffle=False,batch_size=16)\n",
    "music = TestYT(data,model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Οι υπολογισμοί σε όλα τα ερωτήματα έγιναν σε local PC ,με specs CPU: AMD R9 3900x και GPU : NVIDIA RTX 3080."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
