{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project implemented by Konstantinos Lampropoulos $$ $$\n",
    "AM:1115201800092"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Math,Latex\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics.classification import ConfusionMatrix,F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 : FeedForward Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train set\n",
    "X_train = np.load('./music_genre_data_di/train/mfccs/X.npy')\n",
    "Labels_train = np.load('./music_genre_data_di/train/mfccs/labels.npy')\n",
    "\n",
    "#Load Validation set\n",
    "X_val = np.load('./music_genre_data_di/val/mfccs/X.npy')\n",
    "Labels_val = np.load('./music_genre_data_di/val/mfccs/labels.npy')\n",
    "\n",
    "#Load Test set\n",
    "X_test = np.load('./music_genre_data_di/test/mfccs/X.npy')\n",
    "Labels_test = np.load('./music_genre_data_di/test/mfccs/labels.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = LabelEncoder()\n",
    "\n",
    "Labels_train_encoded = encoder.fit_transform(Labels_train)\n",
    "\n",
    "Labels_val_encoded = encoder.fit_transform(Labels_val)\n",
    "\n",
    "Labels_test_encoded = encoder.fit_transform(Labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create torch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#Train set\n",
    "X_train_tensor = torch.tensor(X_train,dtype=torch.float32)\n",
    "Labels_train_tensor = torch.tensor(Labels_train_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_train_tensor,Labels_train_tensor)\n",
    "Train_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#Val set\n",
    "X_val_tensor = torch.tensor(X_val,dtype=torch.float32)\n",
    "Labels_val_tensor = torch.tensor(Labels_val_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_val_tensor,Labels_val_tensor)\n",
    "Validation_set = DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#Test set\n",
    "X_test_tensor = torch.tensor(X_test,dtype=torch.float32)\n",
    "Labels_test_tensor = torch.tensor(Labels_test_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_test_tensor,Labels_test_tensor)\n",
    "Test_set = DataLoader(Dataset,batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device=\"cuda\"\n",
    "else:\n",
    "   device=\"cpu\"\n",
    "print(\"Device =\",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feed Forward Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(26,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = FeedForwardNeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(dataLoader,model,loss_fn,optimizer,scheduler1 = None,scheduler2 = None):\n",
    "    size = len(dataLoader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        #Load to GPU\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        #Prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        #BackPropagation\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    if scheduler1:\n",
    "        scheduler1.step()\n",
    "    if scheduler2:\n",
    "        scheduler2.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(dataloader,model,loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss,correct,f1 = 0,0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            #Load to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            f1_score = F1Score(task='multiclass',num_classes=4,average='macro').to(device)\n",
    "            f1 += f1_score(pred.argmax(1),y)\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    f1 /= size\n",
    "    confmat = ConfusionMatrix('multiclass',num_classes=4).to(device)\n",
    "    confusion_matrix = confmat(pred.argmax(1),y)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f},Avg F1-Score: {f1:>8f}\\n\")\n",
    "    print(f\"Confusion Matrix \\n: {confusion_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(dataloader,model):\n",
    "    size = len(dataloader.dataset)\n",
    "    f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            #Load to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            f1_score = F1Score(task='multiclass',num_classes=4,average='macro').to(device)\n",
    "            f1 += f1_score(pred.argmax(1),y)\n",
    "    f1 /= size\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_f1 = 0\n",
    "learning_rate = 0.002\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "epochs_count = 30\n",
    "for t in range(epochs_count):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    Train(Train_set,model,loss_fn,optimizer)\n",
    "    f1 = Validation(Validation_set,model)\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "    Test(Test_set,model,loss_fn)\n",
    "print(\"Best Model computed by finding the model with the highest f1 score on the validation set\\n\")\n",
    "Test(Test_set,best_model,loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU vs CPU runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Με την χρήση CPU για την αξιολόγηση του μοντέλου πετυχαίνουμε 60.6% accuracy,Loss 0.060977 και f1  0.011394 σε 6.5s\n",
    "* Με την χρήση GPU για την αξιολόγιση του μοντέλου πετυχαίνουμε 62.4% accuracy,Loss 0.061011 και f1  0.011620 σε 22.4s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 : Convolutional Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "import torch.backends.cudnn\n",
    "\n",
    "SEED = 12345\n",
    "rs = RandomState(MT19937(SeedSequence(SEED)))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "g_cuda = torch.Generator(device=device)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train set\n",
    "X_train = np.load('./music_genre_data_di/train/melgrams/X.npy')\n",
    "Labels_train = np.load('./music_genre_data_di/train/melgrams/labels.npy')\n",
    "\n",
    "#Load Validation set\n",
    "X_val = np.load('./music_genre_data_di/val/melgrams/X.npy')\n",
    "Labels_val = np.load('./music_genre_data_di/val/melgrams/labels.npy')\n",
    "\n",
    "#Load Test set\n",
    "X_test = np.load('./music_genre_data_di/test/melgrams/X.npy')\n",
    "Labels_test = np.load('./music_genre_data_di/test/melgrams/labels.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = LabelEncoder()\n",
    "\n",
    "Labels_train_encoded = encoder.fit_transform(Labels_train)\n",
    "\n",
    "Labels_val_encoded = encoder.fit_transform(Labels_val)\n",
    "\n",
    "Labels_test_encoded = encoder.fit_transform(Labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Torch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "#Train set\n",
    "X_train_tensor = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
    "Labels_train_tensor = torch.tensor(Labels_train_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_train_tensor,Labels_train_tensor)\n",
    "Train_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)\n",
    "\n",
    "#Val set\n",
    "X_val_tensor = torch.tensor(X_val,dtype=torch.float32)\n",
    "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
    "Labels_val_tensor = torch.tensor(Labels_val_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_val_tensor,Labels_val_tensor)\n",
    "Validation_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)\n",
    "\n",
    "#Test set\n",
    "X_test_tensor = torch.tensor(X_test,dtype=torch.float32)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "Labels_test_tensor = torch.tensor(Labels_test_encoded,dtype=torch.long)\n",
    "Dataset = TensorDataset(X_test_tensor,Labels_test_tensor)\n",
    "Test_set = DataLoader(Dataset,batch_size=batch_size,worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Convolutional Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        kernel_size, padding = 5, 2\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=kernel_size,padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=kernel_size,padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size,padding=padding)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel_size,padding=padding)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1024, 1024)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.elu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool(F.elu(self.bn2(self.conv2(x))))\n",
    "        x = self.maxpool(F.elu(self.bn3(self.conv3(x))))\n",
    "        x = self.maxpool(F.elu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x,1)\n",
    "        # Fully connected layers\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "model = ConvolutionalNeuralNetwork(dropout_rate=0.5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.354813  [    0/ 3200]\n",
      "loss: 11.222861  [  800/ 3200]\n",
      "loss: 9.694426  [ 1600/ 3200]\n",
      "loss: 8.319596  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.9995e-03.\n",
      "Adjusting learning rate of group 0 to 1.7996e-03.\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 11.757658  [    0/ 3200]\n",
      "loss: 7.089171  [  800/ 3200]\n",
      "loss: 7.519851  [ 1600/ 3200]\n",
      "loss: 6.419196  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.7982e-03.\n",
      "Adjusting learning rate of group 0 to 1.6184e-03.\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 9.046476  [    0/ 3200]\n",
      "loss: 6.026916  [  800/ 3200]\n",
      "loss: 6.436285  [ 1600/ 3200]\n",
      "loss: 5.650746  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.6164e-03.\n",
      "Adjusting learning rate of group 0 to 1.4548e-03.\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 8.185740  [    0/ 3200]\n",
      "loss: 5.720587  [  800/ 3200]\n",
      "loss: 6.261148  [ 1600/ 3200]\n",
      "loss: 5.298303  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.4523e-03.\n",
      "Adjusting learning rate of group 0 to 1.3070e-03.\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 7.388478  [    0/ 3200]\n",
      "loss: 4.725387  [  800/ 3200]\n",
      "loss: 5.941855  [ 1600/ 3200]\n",
      "loss: 5.015018  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.3041e-03.\n",
      "Adjusting learning rate of group 0 to 1.1737e-03.\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 7.113314  [    0/ 3200]\n",
      "loss: 4.236346  [  800/ 3200]\n",
      "loss: 6.118770  [ 1600/ 3200]\n",
      "loss: 4.613506  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.1705e-03.\n",
      "Adjusting learning rate of group 0 to 1.0535e-03.\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 7.514018  [    0/ 3200]\n",
      "loss: 3.808443  [  800/ 3200]\n",
      "loss: 5.311612  [ 1600/ 3200]\n",
      "loss: 4.310164  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.0501e-03.\n",
      "Adjusting learning rate of group 0 to 9.4507e-04.\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 6.418449  [    0/ 3200]\n",
      "loss: 4.318952  [  800/ 3200]\n",
      "loss: 5.825713  [ 1600/ 3200]\n",
      "loss: 4.240253  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 9.4157e-04.\n",
      "Adjusting learning rate of group 0 to 8.4741e-04.\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 5.999190  [    0/ 3200]\n",
      "loss: 4.581290  [  800/ 3200]\n",
      "loss: 4.875782  [ 1600/ 3200]\n",
      "loss: 4.234695  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 8.4384e-04.\n",
      "Adjusting learning rate of group 0 to 7.5946e-04.\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 6.171499  [    0/ 3200]\n",
      "loss: 4.875412  [  800/ 3200]\n",
      "loss: 5.836778  [ 1600/ 3200]\n",
      "loss: 4.073443  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 7.5588e-04.\n",
      "Adjusting learning rate of group 0 to 6.8029e-04.\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 7.454748  [    0/ 3200]\n",
      "loss: 4.329220  [  800/ 3200]\n",
      "loss: 5.064062  [ 1600/ 3200]\n",
      "loss: 3.921699  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 6.7674e-04.\n",
      "Adjusting learning rate of group 0 to 6.0907e-04.\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 8.644501  [    0/ 3200]\n",
      "loss: 3.979176  [  800/ 3200]\n",
      "loss: 4.074854  [ 1600/ 3200]\n",
      "loss: 3.884423  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 6.0558e-04.\n",
      "Adjusting learning rate of group 0 to 5.4503e-04.\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 8.586308  [    0/ 3200]\n",
      "loss: 3.807106  [  800/ 3200]\n",
      "loss: 3.562846  [ 1600/ 3200]\n",
      "loss: 3.838937  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 5.4163e-04.\n",
      "Adjusting learning rate of group 0 to 4.8747e-04.\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 7.717550  [    0/ 3200]\n",
      "loss: 3.842724  [  800/ 3200]\n",
      "loss: 3.343825  [ 1600/ 3200]\n",
      "loss: 4.041022  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.8418e-04.\n",
      "Adjusting learning rate of group 0 to 4.3576e-04.\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 6.534320  [    0/ 3200]\n",
      "loss: 3.781996  [  800/ 3200]\n",
      "loss: 3.423560  [ 1600/ 3200]\n",
      "loss: 3.989466  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.3260e-04.\n",
      "Adjusting learning rate of group 0 to 3.8934e-04.\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 5.866490  [    0/ 3200]\n",
      "loss: 3.559727  [  800/ 3200]\n",
      "loss: 3.642447  [ 1600/ 3200]\n",
      "loss: 3.880185  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.8631e-04.\n",
      "Adjusting learning rate of group 0 to 3.4768e-04.\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 5.387861  [    0/ 3200]\n",
      "loss: 3.537161  [  800/ 3200]\n",
      "loss: 3.473343  [ 1600/ 3200]\n",
      "loss: 3.997314  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.4480e-04.\n",
      "Adjusting learning rate of group 0 to 3.1032e-04.\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 5.371764  [    0/ 3200]\n",
      "loss: 3.747932  [  800/ 3200]\n",
      "loss: 3.140165  [ 1600/ 3200]\n",
      "loss: 3.866088  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.0758e-04.\n",
      "Adjusting learning rate of group 0 to 2.7682e-04.\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 5.707227  [    0/ 3200]\n",
      "loss: 3.574354  [  800/ 3200]\n",
      "loss: 2.951727  [ 1600/ 3200]\n",
      "loss: 3.487164  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.7423e-04.\n",
      "Adjusting learning rate of group 0 to 2.4681e-04.\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 5.737418  [    0/ 3200]\n",
      "loss: 3.160964  [  800/ 3200]\n",
      "loss: 2.843873  [ 1600/ 3200]\n",
      "loss: 3.171943  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.4437e-04.\n",
      "Adjusting learning rate of group 0 to 2.1993e-04.\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 5.351760  [    0/ 3200]\n",
      "loss: 2.761428  [  800/ 3200]\n",
      "loss: 2.717140  [ 1600/ 3200]\n",
      "loss: 3.002730  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.1764e-04.\n",
      "Adjusting learning rate of group 0 to 1.9588e-04.\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 4.708677  [    0/ 3200]\n",
      "loss: 2.385547  [  800/ 3200]\n",
      "loss: 2.566807  [ 1600/ 3200]\n",
      "loss: 2.921985  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.9373e-04.\n",
      "Adjusting learning rate of group 0 to 1.7435e-04.\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.987458  [    0/ 3200]\n",
      "loss: 2.016040  [  800/ 3200]\n",
      "loss: 2.407632  [ 1600/ 3200]\n",
      "loss: 2.858727  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.7235e-04.\n",
      "Adjusting learning rate of group 0 to 1.5511e-04.\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.308891  [    0/ 3200]\n",
      "loss: 1.654242  [  800/ 3200]\n",
      "loss: 2.244089  [ 1600/ 3200]\n",
      "loss: 2.777771  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.5324e-04.\n",
      "Adjusting learning rate of group 0 to 1.3791e-04.\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.726359  [    0/ 3200]\n",
      "loss: 1.323238  [  800/ 3200]\n",
      "loss: 2.077662  [ 1600/ 3200]\n",
      "loss: 2.668593  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.3617e-04.\n",
      "Adjusting learning rate of group 0 to 1.2255e-04.\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.252548  [    0/ 3200]\n",
      "loss: 1.046383  [  800/ 3200]\n",
      "loss: 1.910125  [ 1600/ 3200]\n",
      "loss: 2.531584  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.2093e-04.\n",
      "Adjusting learning rate of group 0 to 1.0884e-04.\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.887509  [    0/ 3200]\n",
      "loss: 0.825157  [  800/ 3200]\n",
      "loss: 1.743447  [ 1600/ 3200]\n",
      "loss: 2.372797  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.0734e-04.\n",
      "Adjusting learning rate of group 0 to 9.6605e-05.\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.614345  [    0/ 3200]\n",
      "loss: 0.653563  [  800/ 3200]\n",
      "loss: 1.583663  [ 1600/ 3200]\n",
      "loss: 2.199414  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 9.5216e-05.\n",
      "Adjusting learning rate of group 0 to 8.5694e-05.\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.417234  [    0/ 3200]\n",
      "loss: 0.521730  [  800/ 3200]\n",
      "loss: 1.438438  [ 1600/ 3200]\n",
      "loss: 2.020771  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 8.4411e-05.\n",
      "Adjusting learning rate of group 0 to 7.5970e-05.\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.279949  [    0/ 3200]\n",
      "loss: 0.423631  [  800/ 3200]\n",
      "loss: 1.315663  [ 1600/ 3200]\n",
      "loss: 1.843818  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 7.4787e-05.\n",
      "Adjusting learning rate of group 0 to 6.7308e-05.\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.186577  [    0/ 3200]\n",
      "loss: 0.350664  [  800/ 3200]\n",
      "loss: 1.216175  [ 1600/ 3200]\n",
      "loss: 1.676168  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 6.6219e-05.\n",
      "Adjusting learning rate of group 0 to 5.9597e-05.\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.126247  [    0/ 3200]\n",
      "loss: 0.296535  [  800/ 3200]\n",
      "loss: 1.135792  [ 1600/ 3200]\n",
      "loss: 1.522128  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 5.8595e-05.\n",
      "Adjusting learning rate of group 0 to 5.2735e-05.\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.088784  [    0/ 3200]\n",
      "loss: 0.255933  [  800/ 3200]\n",
      "loss: 1.071498  [ 1600/ 3200]\n",
      "loss: 1.383923  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 5.1816e-05.\n",
      "Adjusting learning rate of group 0 to 4.6634e-05.\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.065527  [    0/ 3200]\n",
      "loss: 0.225543  [  800/ 3200]\n",
      "loss: 1.019069  [ 1600/ 3200]\n",
      "loss: 1.262824  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.5791e-05.\n",
      "Adjusting learning rate of group 0 to 4.1212e-05.\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.050307  [    0/ 3200]\n",
      "loss: 0.202609  [  800/ 3200]\n",
      "loss: 0.976146  [ 1600/ 3200]\n",
      "loss: 1.159397  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.0440e-05.\n",
      "Adjusting learning rate of group 0 to 3.6396e-05.\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.039386  [    0/ 3200]\n",
      "loss: 0.185211  [  800/ 3200]\n",
      "loss: 0.940027  [ 1600/ 3200]\n",
      "loss: 1.072370  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.5689e-05.\n",
      "Adjusting learning rate of group 0 to 3.2121e-05.\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.030239  [    0/ 3200]\n",
      "loss: 0.171787  [  800/ 3200]\n",
      "loss: 0.909173  [ 1600/ 3200]\n",
      "loss: 0.999447  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.1476e-05.\n",
      "Adjusting learning rate of group 0 to 2.8328e-05.\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.023004  [    0/ 3200]\n",
      "loss: 0.161146  [  800/ 3200]\n",
      "loss: 0.882853  [ 1600/ 3200]\n",
      "loss: 0.938695  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.7739e-05.\n",
      "Adjusting learning rate of group 0 to 2.4966e-05.\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.015021  [    0/ 3200]\n",
      "loss: 0.152707  [  800/ 3200]\n",
      "loss: 0.859931  [ 1600/ 3200]\n",
      "loss: 0.888671  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.4429e-05.\n",
      "Adjusting learning rate of group 0 to 2.1986e-05.\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.007524  [    0/ 3200]\n",
      "loss: 0.145964  [  800/ 3200]\n",
      "loss: 0.840058  [ 1600/ 3200]\n",
      "loss: 0.847345  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.1498e-05.\n",
      "Adjusting learning rate of group 0 to 1.9348e-05.\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.000518  [    0/ 3200]\n",
      "loss: 0.140536  [  800/ 3200]\n",
      "loss: 0.822773  [ 1600/ 3200]\n",
      "loss: 0.812542  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.8905e-05.\n",
      "Adjusting learning rate of group 0 to 1.7014e-05.\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.993906  [    0/ 3200]\n",
      "loss: 0.136002  [  800/ 3200]\n",
      "loss: 0.808070  [ 1600/ 3200]\n",
      "loss: 0.784144  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.6611e-05.\n",
      "Adjusting learning rate of group 0 to 1.4950e-05.\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.987709  [    0/ 3200]\n",
      "loss: 0.132234  [  800/ 3200]\n",
      "loss: 0.795572  [ 1600/ 3200]\n",
      "loss: 0.760345  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.4584e-05.\n",
      "Adjusting learning rate of group 0 to 1.3126e-05.\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.981776  [    0/ 3200]\n",
      "loss: 0.129134  [  800/ 3200]\n",
      "loss: 0.784339  [ 1600/ 3200]\n",
      "loss: 0.740601  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.2794e-05.\n",
      "Adjusting learning rate of group 0 to 1.1515e-05.\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.976585  [    0/ 3200]\n",
      "loss: 0.126522  [  800/ 3200]\n",
      "loss: 0.774901  [ 1600/ 3200]\n",
      "loss: 0.723842  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.1215e-05.\n",
      "Adjusting learning rate of group 0 to 1.0093e-05.\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.972075  [    0/ 3200]\n",
      "loss: 0.124442  [  800/ 3200]\n",
      "loss: 0.766692  [ 1600/ 3200]\n",
      "loss: 0.709695  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 9.8219e-06.\n",
      "Adjusting learning rate of group 0 to 8.8397e-06.\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.967718  [    0/ 3200]\n",
      "loss: 0.122669  [  800/ 3200]\n",
      "loss: 0.759675  [ 1600/ 3200]\n",
      "loss: 0.697871  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 8.5944e-06.\n",
      "Adjusting learning rate of group 0 to 7.7350e-06.\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.964164  [    0/ 3200]\n",
      "loss: 0.121151  [  800/ 3200]\n",
      "loss: 0.753667  [ 1600/ 3200]\n",
      "loss: 0.687523  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 7.5136e-06.\n",
      "Adjusting learning rate of group 0 to 6.7622e-06.\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.960794  [    0/ 3200]\n",
      "loss: 0.119998  [  800/ 3200]\n",
      "loss: 0.748560  [ 1600/ 3200]\n",
      "loss: 0.678715  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 6.5625e-06.\n",
      "Adjusting learning rate of group 0 to 5.9063e-06.\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.957705  [    0/ 3200]\n",
      "loss: 0.118888  [  800/ 3200]\n",
      "loss: 0.744114  [ 1600/ 3200]\n",
      "loss: 0.671453  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 5.7264e-06.\n",
      "Adjusting learning rate of group 0 to 5.1538e-06.\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.955045  [    0/ 3200]\n",
      "loss: 0.117938  [  800/ 3200]\n",
      "loss: 0.740137  [ 1600/ 3200]\n",
      "loss: 0.665039  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.9919e-06.\n",
      "Adjusting learning rate of group 0 to 4.4927e-06.\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.953088  [    0/ 3200]\n",
      "loss: 0.117134  [  800/ 3200]\n",
      "loss: 0.736865  [ 1600/ 3200]\n",
      "loss: 0.659880  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 4.3472e-06.\n",
      "Adjusting learning rate of group 0 to 3.9124e-06.\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.951232  [    0/ 3200]\n",
      "loss: 0.116531  [  800/ 3200]\n",
      "loss: 0.734326  [ 1600/ 3200]\n",
      "loss: 0.655447  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.7817e-06.\n",
      "Adjusting learning rate of group 0 to 3.4035e-06.\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.949521  [    0/ 3200]\n",
      "loss: 0.115903  [  800/ 3200]\n",
      "loss: 0.731941  [ 1600/ 3200]\n",
      "loss: 0.651388  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 3.2862e-06.\n",
      "Adjusting learning rate of group 0 to 2.9576e-06.\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.948217  [    0/ 3200]\n",
      "loss: 0.115458  [  800/ 3200]\n",
      "loss: 0.729747  [ 1600/ 3200]\n",
      "loss: 0.648120  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.8524e-06.\n",
      "Adjusting learning rate of group 0 to 2.5672e-06.\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.947050  [    0/ 3200]\n",
      "loss: 0.115048  [  800/ 3200]\n",
      "loss: 0.728015  [ 1600/ 3200]\n",
      "loss: 0.645200  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.4730e-06.\n",
      "Adjusting learning rate of group 0 to 2.2257e-06.\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.945962  [    0/ 3200]\n",
      "loss: 0.114691  [  800/ 3200]\n",
      "loss: 0.726398  [ 1600/ 3200]\n",
      "loss: 0.642739  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 2.1414e-06.\n",
      "Adjusting learning rate of group 0 to 1.9273e-06.\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.945049  [    0/ 3200]\n",
      "loss: 0.114385  [  800/ 3200]\n",
      "loss: 0.725104  [ 1600/ 3200]\n",
      "loss: 0.640627  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.8520e-06.\n",
      "Adjusting learning rate of group 0 to 1.6668e-06.\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.944232  [    0/ 3200]\n",
      "loss: 0.114147  [  800/ 3200]\n",
      "loss: 0.723969  [ 1600/ 3200]\n",
      "loss: 0.638935  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.5996e-06.\n",
      "Adjusting learning rate of group 0 to 1.4396e-06.\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.943611  [    0/ 3200]\n",
      "loss: 0.113897  [  800/ 3200]\n",
      "loss: 0.723162  [ 1600/ 3200]\n",
      "loss: 0.637414  [ 2400/ 3200]\n",
      "Adjusting learning rate of group 0 to 1.3797e-06.\n",
      "Adjusting learning rate of group 0 to 1.2417e-06.\n",
      "Best Model computed by finding the model with the highest f1 score on the validation set\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.087833,Avg F1-Score: 0.024948\n",
      "\n",
      "Confusion Matrix \n",
      ": tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [1, 0, 0, 7]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_f1 = 0\n",
    "learning_rate = 2e-3\n",
    "patience = 3\n",
    "epochs_count = 60\n",
    "patience_tol = epochs_count\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=learning_rate,weight_decay=1e-7)\n",
    "scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,verbose=True,T_max=100)\n",
    "scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
    "for t in range(epochs_count):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    torch.cuda.synchronize()\n",
    "    Train(Train_set,model,loss_fn,optimizer,scheduler1,scheduler2)\n",
    "    f1 = Validation(Validation_set,model)\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "    else:\n",
    "        patience+=1\n",
    "    if patience == patience_tol:\n",
    "        break\n",
    "print(\"Best Model computed by finding the model with the highest f1 score on the validation set\\n\")\n",
    "Test(Test_set,best_model,loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU vs CPU runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Στο GPU έτρεξε σε 1m1.6s και επέστρεψε βέλτιστο μοντέλο με accuracy 65.6% ,Avg loss: 0.132755,Avg F1-Score: 0.012216\n",
    "* Στο CPU έτρεξε σε χρόνο 26m25.1s και επέστρεψε βέλτιστο μοντέλο με  Accuracy: 65.0%, Avg loss: 0.152512,Avg F1-Score: 0.012084"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling And Padding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Με max pooling με kernel = 2,padding = 2 η επίδοση αυξήθηκε στο 70.9% σε χρόνο 40.7s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Optimizer} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{SGD} & \\text{70.9\\%} & \\text{0.012768}\\\\\n",
    "\\hline\n",
    "\\text{Adadelta} & \\text{74.5\\%} & \\text{0.013245}\\\\\n",
    "\\hline\n",
    "\\text{Adagrad} & \\text{75.1\\%} & \\text{0.013355}\\\\\n",
    "\\hline\n",
    "\\text{Adam} & \\text{70.7\\%} & \\text{0.013004}\\\\\n",
    "\\hline\n",
    "\\text{AdamW} & \\text{69.5\\%} & \\text{0.012643}\\\\\n",
    "\\hline\n",
    "\\text{Adamax} & \\text{72.2\\%} & \\text{0.013016}\\\\\n",
    "\\hline\n",
    "\\text{ASGD} & \\text{72.2\\%} & \\text{0.013016}\\\\\n",
    "\\hline\n",
    "\\text{NAdam} & \\text{68.3\\%} & \\text{0.012598}\\\\\n",
    "\\hline\n",
    "\\text{RAdam} & \\text{69.0\\%} & \\text{0.012522}\\\\\n",
    "\\hline\n",
    "\\text{RMsprop} & \\text{70.9\\%} & \\text{0.012781}\\\\\n",
    "\\hline\n",
    "\\text{Rprop} & \\text{71.3\\%} & \\text{0.012988}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers Comparsion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Την καλύτερη απόδοση την είχε ο αλγόριθμος Adagrad που πέτυχε accuracy 75.1% με F1-Score 0.013355.Γενικά η απόκλιση των optimizers είναι μικρή με το μικρότερο accuracy να είναι του NAdam με 68.3% και το μέγιστο του AdaGrad με 75.1%.Οι αποκλίσεις αυτές εξαρτώνται και από το shuffling που υπαρχει στα train και validation sets αλλά και στο ότι κάποιοι αλγόριθμοι αποδίδουν καλύτερα σε διαφορετικά προβλήματα από κάποιους άλλους."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 : Improving Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Activator} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{ReLu} & \\text{32.0\\%} & \\text{0.005272}\\\\\n",
    "\\hline\n",
    "\\text{ELU} & \\text{38.9\\%} & \\text{0.006511}\\\\\n",
    "\\hline\n",
    "\\text{HardShrink} & \\text{33.1\\%} & \\text{0.005456}\\\\\n",
    "\\hline\n",
    "\\text{HardSigmoid} & \\text{23.5\\%} & \\text{0.003706}\\\\\n",
    "\\hline\n",
    "\\text{HardTanh} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{RReLU} & \\text{30.8\\%} & \\text{0.005007}\\\\\n",
    "\\hline\n",
    "\\text{GELU} & \\text{32.3\\%} & \\text{0.005344}\\\\\n",
    "\\hline\n",
    "\\text{SoftPlus} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Scheduler} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{LinearLR} & \\text{30.5\\%} & \\text{0.004912}\\\\\n",
    "\\hline\n",
    "\\text{PolynomialLR} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{ExponentialLR} & \\text{63.7\\%} & \\text{0.011358}\\\\\n",
    "\\hline\n",
    "\\text{MultiStepLR} & \\text{38.9\\%} & \\text{0.006511}\\\\\n",
    "\\hline\n",
    "\\text{CosineAnnealingLR} & \\text{63.3\\%} & \\text{0.011417}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Accuracy: 73.3%, Avg loss: 0.062259,Avg F1-Score: 0.012951"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight Decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Weight Decay} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{0.5} & \\text{29.0\\%} & \\text{0.004536}\\\\\n",
    "\\hline\n",
    "\\text{1e-2} & \\text{52.3\\%} & \\text{0.009304}\\\\\n",
    "\\hline\n",
    "\\text{1e-3} & \\text{51.9\\%} & \\text{0.009138}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{61.9\\%} & \\text{0.010879}\\\\\n",
    "\\hline\n",
    "\\text{1e-5} & \\text{57.6\\%} & \\text{0.010176}\\\\\n",
    "\\hline\n",
    "\\text{1e-7} & \\text{55.6\\%} & \\text{0.009886}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\text{Dropout Rate} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{0.5} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{0.2} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\text{0.1} & \\text{75.4\\%} & \\text{0.013121}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight Decay and Dropout Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Weight Decay} & \\text{Dropout Rate} & \\text{Accuracy} & \\text{F1-Score}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.1} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.2} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.5} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{0.7} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\text{1e-4} & \\text{1.0} & \\text{63.2\\%} & \\text{0.011532}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Batch Size} & \\text{Accuracy} & \\text{F1-Score} & \\text{Time}\\\\\n",
    "\\hline\n",
    "\\text{2} & \\text{66.6\\%} & \\text{0.089510} & \\text{9m26.8s}\\\\\n",
    "\\hline\n",
    "\\text{4} & \\text{71.7\\%} & \\text{0.048821} & \\text{5m25.7s}\\\\\n",
    "\\hline\n",
    "\\text{8} & \\text{72.3\\%} & \\text{0.024948} & \\text{2m50.1s}\\\\\n",
    "\\hline\n",
    "\\text{16} & \\text{63.2\\%} & \\text{0.011532} & \\text{1m17.9s}\\\\\n",
    "\\hline\n",
    "\\text{32} & \\text{53.3\\%} & \\text{0.005186} & \\text{39.9s}\\\\\n",
    "\\hline\n",
    "\\text{64} & \\text{53.9\\%} & \\text{0.015037} & \\text{22.7s}\\\\\n",
    "\\hline\n",
    "\\text{128} & \\text{51.3\\%} & \\text{0.001545} & \\text{13.5s}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Παρατηρούμε ότι όσο αυξάνεται το batch size μέχρι το 8 ,αυξάνεται και το accuracy ,ενώ υποδιπλασιάζονται τα F1-score και ο χρόνος εκτέλεσης.Υπό τις βέλτιστες συνθήκες το καλύτερο Batch Size θα ήταν 4(δηλαδή χωρίς να λάβουμε υπόψιν τον χρόνο εκτέλεσης).Όμως μιάς και ο χρόνος εκτέλεσης μας νοιάζει,το βέλτιστο batch size είναι το 8.Πέραν αυτού μειώνεται και το accuracy κατα πολύ,και το χρονικό κέρδος δεν αξίζει για την μείωση της απόδοσης του μοντέλου."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Patience} & \\text{Accuracy} & \\text{F1-Score} & \\text{Time}\\\\\n",
    "\\hline\n",
    "\\text{3} & \\text{73.2\\%} & \\text{0.012941} & \\text{43.3s}\\\\\n",
    "\\hline\n",
    "\\text{5} & \\text{73.1\\%} & \\text{0.012935} & \\text{44.9s}\\\\\n",
    "\\hline\n",
    "\\text{6} & \\text{73.8\\%} & \\text{0.012984} & \\text{49.3s}\\\\\n",
    "\\hline\n",
    "\\text{7} & \\text{74.2\\%} & \\text{0.013015} & \\text{49.8s}\\\\\n",
    "\\hline\n",
    "\\text{10} & \\text{72.7\\%} & \\text{0.012781} & \\text{54.5s}\\\\\n",
    "\\hline\n",
    "\\text{15} & \\text{72.8\\%} & \\text{0.012746} & \\text{1m2.6s}\\\\\n",
    "\\hline\n",
    "\\text{None} & \\text{75.4\\%} & \\text{0.013121} & \\text{1m20.5s}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Παρατηρώντας τα δεδομένα,αν θέλουμε να θέσουμε ένα patience για το δεδομένο μοντέλο,το βέλτιστο θα ήταν 7.Όμως αφού ο χρόνος ήδη δεν είναι πολύ απαιτητικός θα μπορούσαμε να μην θέσουμε."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_genre_data_di.youtube import youtube_to_melgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_Type(type):\n",
    "    if type == 0 : type_str = 'blues'\n",
    "    elif type == 1 : type_str = 'classical'\n",
    "    elif type == 2 : type_str = 'hiphop'\n",
    "    else : type_str = 'rock_metal_hardrock'\n",
    "    return type_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestYT(dataloader,model):\n",
    "    music_type = []\n",
    "    music_percentage = {}\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            music_type.append(pred.argmax(1))\n",
    "    for tensor in music_type:\n",
    "        unique_types, counts = torch.unique(tensor, return_counts=True)\n",
    "        for type, count in zip(unique_types, counts):\n",
    "            if type.item() in music_percentage:\n",
    "                music_percentage[type.item()] += count.item()\n",
    "            else:\n",
    "                music_percentage[type.item()] = count.item()\n",
    "    total_count = sum(music_percentage.values())\n",
    "    percentages = {type: count / total_count * 100 for type, count in music_percentage.items()}              \n",
    "    for type, percentage in percentages.items():\n",
    "        print(f'Type: {Map_Type(type)}, Percentage: {percentage:.2f}%')\n",
    "    return music_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_to_melgram(url = 'https://www.youtube.com/watch?v=9E6b3swbnWg',filename=\"./Classical.wav\")\n",
    "youtube_to_melgram(url = \"https://www.youtube.com/watch?v=EDwb9jOVRtU\",filename=\"./HipHop.wav\")\n",
    "youtube_to_melgram(url = \"https://www.youtube.com/watch?v=OMaycNcPsHI\",filename=\"./Rock.wav\")\n",
    "youtube_to_melgram(url = \"https://www.youtube.com/watch?v=l45f28PzfCI\",filename=\"./Blues.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Post-Processor arguments given without specifying name. The arguments will be given to all post-processors\n"
     ]
    }
   ],
   "source": [
    "youtube_to_melgram(url = \"https://www.youtube.com/watch?v=V5Ar0dKnl6Y\",filename=\"./Fun.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: classical, Percentage: 98.15%\n",
      "Type: rock_metal_hardrock, Percentage: 1.11%\n",
      "Type: blues, Percentage: 0.37%\n",
      "Type: hiphop, Percentage: 0.37%\n"
     ]
    }
   ],
   "source": [
    "X = np.load('./Classical.npy')\n",
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = torch.full((X_tensor.size(0),),0,dtype=torch.long)\n",
    "tensor_dataset = TensorDataset(X_tensor,y_tensor)\n",
    "data = DataLoader(tensor_dataset,shuffle=False,batch_size=8)\n",
    "music = TestYT(data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: hiphop, Percentage: 53.89%\n",
      "Type: rock_metal_hardrock, Percentage: 29.94%\n",
      "Type: blues, Percentage: 9.88%\n",
      "Type: classical, Percentage: 6.29%\n"
     ]
    }
   ],
   "source": [
    "X = np.load('./HipHop.npy')\n",
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = torch.full((X_tensor.size(0),),0,dtype=torch.long)\n",
    "tensor_dataset = TensorDataset(X_tensor,y_tensor)\n",
    "data = DataLoader(tensor_dataset,shuffle=False,batch_size=8)\n",
    "music = TestYT(data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: blues, Percentage: 22.36%\n",
      "Type: classical, Percentage: 8.63%\n",
      "Type: hiphop, Percentage: 14.70%\n",
      "Type: rock_metal_hardrock, Percentage: 54.31%\n"
     ]
    }
   ],
   "source": [
    "X = np.load('./Blues.npy')\n",
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = torch.full((X_tensor.size(0),),0,dtype=torch.long)\n",
    "tensor_dataset = TensorDataset(X_tensor,y_tensor)\n",
    "data = DataLoader(tensor_dataset,shuffle=False,batch_size=8)\n",
    "music = TestYT(data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: classical, Percentage: 4.17%\n",
      "Type: rock_metal_hardrock, Percentage: 64.35%\n",
      "Type: blues, Percentage: 26.39%\n",
      "Type: hiphop, Percentage: 5.09%\n"
     ]
    }
   ],
   "source": [
    "X = np.load('./Rock.npy')\n",
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = torch.full((X_tensor.size(0),),0,dtype=torch.long)\n",
    "tensor_dataset = TensorDataset(X_tensor,y_tensor)\n",
    "data = DataLoader(tensor_dataset,shuffle=False,batch_size=8)\n",
    "music = TestYT(data,model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Από τα παραπάνω αποτελέσματα φαίνεται ότι το δίκτυο αναγνώρισε σωστά το είδος μουσικής του Nocturne op.9 No.2 του Choplin,αφού το προσδιόρισε κατα 98% Classical\n",
    "* Προσδιόρισε επίσης σωστά το τραγούδι της Madonna,Hung up,ως Hip-Hop κατά 53.89%.\n",
    "* Προσδιόρισε λάθος το τραγούδι του BB King,How Blue Can You Get,τ'οποίο εν τέλει το έθεσε στην κατηγορία Rock,ενώ το τραγούδι είναι Blues.\n",
    "* Τέλος το τραγούδι των Placebo ,Every You Every Me,το προσδιόρισε ορθά ως Rock κατά 64.35% ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Λεπτομέρειες"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Οι υπολογισμοί σε όλα τα ερωτήματα έγιναν σε local PC ,με specs CPU: AMD R9 3900x και GPU : NVIDIA RTX 3080,επομένως οι χρόνοι εκτέλεσης μπορεί να διαφέρουν αρκετά από τους χρόνους εκτέλεσης του Notebook στο Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
