{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Λαμπρόπουλος Κωνσταντίνος $$ $$\n",
    "ΑΜ : 1115201800092"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning $$ $$\n",
    "Εργασία 1η :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Math,Latex\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(92) #Set a seed\n",
    "x = np.random.randint(10,size=(3,4)) #create an array 3x4\n",
    "y = np.random.randint(10,size=(4,3)) #create an array 4x3\n",
    "print(x) #print both arrays\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(92) #set the random seed\n",
    "a = np.random.randint(10,size = 4) #create 2 random vectors\n",
    "b = np.random.randint(10,size = 4)\n",
    "print(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.inner(a,b) #calculate the inner product of the 2 vectors\n",
    "print(c) #print it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{A} = [3,2,6,7],\\vec{B} = [1,4,9,0]\\\\\n",
    "$$\n",
    "Και έχουμε ότι :\n",
    "$$\n",
    "<\\vec{A},\\vec{B}> = \\sum_{n=1}^{4} x_ny_n \\Leftrightarrow \\\\\n",
    "<\\vec{A},\\vec{B}> = (3\\times 1) + (2\\times 4) + (6\\times 9) + (7\\times 0) = 3 + 8 + 54 = 65\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x@a) #print the product of X and A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε τον πίνακα : \n",
    "$$ \n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "3 & 2 & 6 & 7\\\\\n",
    "1 & 4 & 9 & 0\\\\\n",
    "0 & 2 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "και το δίανυσμα : \n",
    "$$\\vec{A} = [3,2,6,7]$$\n",
    "Ο πολλαπλασιασμός είναι επιτρεπτός μιάς και έχουμε πίνακα 4 στηλών και διάνυσμα 4 σειρών,και έχουμε : \n",
    "$$\n",
    "Χ\\times \\vec{A} = \n",
    "\\begin{bmatrix}\n",
    "    3 & 2 & 6 & 7\\\\\n",
    "    1 & 4 & 9 & 0\\\\\n",
    "    0 & 2 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    3\\\\\n",
    "    2\\\\\n",
    "    6\\\\\n",
    "    7\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    (3\\times 3) + (2\\times 2) + (6\\times 6) + (7\\times 7)\\\\\n",
    "    (1\\times 3) + (4\\times 2) + (9\\times 6) + (0\\times 7)\\\\\n",
    "    (0\\times 3) + (2\\times 2) + (6\\times 7) + (7 \\times 2)\n",
    "\\end{bmatrix}\\\\\n",
    "= \n",
    " \\begin{bmatrix}\n",
    "    9 + 4 + 36 + 49\\\\\n",
    "    3 + 8 + 54 + 0\\\\\n",
    "    0 + 4 + 42 + 14\n",
    "\\end{bmatrix}\n",
    "=\n",
    " \\begin{bmatrix}\n",
    "    98\\\\\n",
    "    65\\\\\n",
    "    60\n",
    "\\end{bmatrix} \\Leftrightarrow \n",
    "\\\\\n",
    "Χ\\times \\vec{A} =  \n",
    "\\begin{bmatrix}\n",
    "    98\\\\\n",
    "    65\\\\\n",
    "    60\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x@y) #print the product of X and Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε τους πίνακες :\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "3 & 2 & 6 & 7\\\\\n",
    "1 & 4 & 9 & 0\\\\\n",
    "0 & 2 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "και\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 1\\\\\n",
    "4 & 1 & 1\\\\\n",
    "8 & 9 & 4\\\\\n",
    "9 & 9 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Ο πολλαπλασιασμός τους είναι επιτρεπτός μιάς και ο X είναι πίνακας 4 στηλών και ο Υ\n",
    "είναι πίνακας 4 σειρών,και έχουμε :\n",
    "$$\n",
    "X\\times Y = \n",
    "\\begin{bmatrix}\n",
    "3 & 2 & 6 & 7\\\\\n",
    "1 & 4 & 9 & 0\\\\\n",
    "0 & 2 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 1\\\\\n",
    "4 & 1 & 1\\\\\n",
    "8 & 9 & 4\\\\\n",
    "9 & 9 & 6\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "(3\\times 2) + (2 \\times 4) + (6\\times 8) + (7\\times 9) & (3\\times 1) + (2\\times 1) + (6 \\times 9) + (7\\times 9) & (3\\times 1) + (2\\times 1) + (6\\times 4) + (7\\times 6)\\\\\n",
    "\n",
    "(1\\times 2) + (4 \\times 4) + (9\\times 8) + (0\\times 9) & (1\\times 1) + (4\\times 1) + (9 \\times 9) + (0\\times 9) & (1\\times 1) + (4\\times 1) + (9\\times 4) + (0\\times 6)\\\\\n",
    "\n",
    "(0\\times 2) + (2 \\times 4) + (7\\times 8) + (2\\times 9) & (0\\times 1) + (2\\times 1) + (7 \\times 9) + (2\\times 9) & (0\\times 1) + (2\\times 1) + (7\\times 4) + (2\\times 6)\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 + 8 + 48 + 63 & 3 + 2+ 54 + 63 & 3 + 2 + 24 + 42\\\\\n",
    "2 + 16 + 72 + 0 & 1 + 4 + 81 + 0 & 1 + 4 + 36 + 0\\\\\n",
    "0 + 8 + 56 + 18 & 0 + 2 + 63 + 18 & 0 + 2 + 28 + 12\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "125 & 122 & 71\\\\\n",
    "90 & 86 & 41\\\\\n",
    "82 & 83 & 42\n",
    "\\end{bmatrix}\n",
    "\\Leftrightarrow\n",
    "\\\\\n",
    "X\\times Y = \n",
    "\\begin{bmatrix}\n",
    "125 & 122 & 71\\\\\n",
    "90 & 86 & 41\\\\\n",
    "82 & 83 & 42\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(a) #find the L2 norm of A\n",
    "print(norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε το διάνυσμα : \n",
    "$$\n",
    "\\vec{A} = [3,2,6,7]\n",
    "$$\n",
    "Για την Ευκλείδια νόρμα του Α έχουμε : \n",
    "$$\n",
    "||A||_2 = \\sqrt{A^T A} = \\sqrt{A_1^2 + A_2^2 + A_3^2 +A_4^2} \\\\\n",
    "= \\sqrt{3^2 + 2^2 + 6^2 + 7^2} = \\sqrt{9 + 4 + 36 + 49} = \\sqrt{98}\n",
    "= 9,899494936611665 \\Leftrightarrow \\\\\n",
    "||A||_2 = 9,899494936611665\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε την συνάρτηση : $$f(x) = x^TAx + b^Tx $$,όπου Α συμμετρικός διάστασης dxd και $$x,b\\in\\mathbb{R^d}$$\n",
    "Έχουμε λοιπόν : \n",
    "$$ \n",
    "    \\frac{\\partial f(x)}{\\partial x} = \\frac{\\partial (x^TAx + b^Tx)}{\\partial x}\n",
    "    = \\frac{\\partial (x^TAx)}{\\partial x} + \\frac{\\partial (b^Tx)}{\\partial x}\n",
    "    = (A + A^T)x + b = 2Ax + b\n",
    "$$\n",
    "αφού γνωρίζουμε ότι ο Α είναι συμμετρικός και άρα $$A = A^T$$\n",
    "Οι μέθοδοι παραγώγισης που χρησιμοποιήθηκαν είναι οι (69) και (81) του matrix cookbook\n",
    "στις σελίδες 10 και 11 αντίστοιχα.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε τους πίνακες A,B,X διάστασης d x d και rank d και θέλουμε το ολικό ελάχιστο του:\n",
    "$$\n",
    "min||A - XB||^{2}_{F} \\hspace{1cm}(1)\n",
    "$$\n",
    "Έχουμε λοιπόν :\n",
    "$$\n",
    "\\frac{\\partial(||A - XB||^{2}_{F})}{\\partial X} = \\frac{\\partial(Tr((A - XB)(A - XB)^H))}{\\partial X}\n",
    "$$\n",
    "όπου : \n",
    "Αν θεωρήσουμε τον πίνακα C = -I (όπου Ι ο μοναδιαίος πίνακας δίαστασης d) η παραπάνω σχέση μετατρέπεται:\n",
    "$$\n",
    "\\frac{\\partial(Tr((A - XB)(A - XB)^H))}{\\partial X} = \\frac{\\partial(Tr((A + CXB)(A +\n",
    "CXB)^H))}{\\partial X} \\\\\n",
    "=\n",
    "\\frac{\\partial(Tr((CXB + A)(CXB +\n",
    "A)^H))}{\\partial X}\n",
    "= \n",
    "2C^T(CXB + A)B^T = 2(-I)^T(-IXB + A)B^T\n",
    "\\\\\n",
    "=\n",
    "-2(-XB + A)B^T \\\\\n",
    "= -2(A - XB)B^T \\hspace{1cm}(2)\n",
    "$$\n",
    "Για να βρούμε το ελάχιστο της (1) αρκεί να βρούμε τις τιμές για τις οποίες η (2) είναι 0.\n",
    "Έχουμε λοιπόν:\n",
    "$$\n",
    "-2(A - XB)B^T = 0 \\Leftrightarrow\n",
    "(A - XB)B^T = 0 \\Leftrightarrow\\\\\n",
    "AB^T - XBB^T = 0 \\Leftrightarrow\n",
    "XBB^T = AB^T \\Leftrightarrow\\\\\n",
    "X = AB^T(BB^T)^{-1} \\Leftrightarrow\n",
    "X = AB^T(B^T)^{-1}B^{-1} \\Leftrightarrow\\\\\n",
    "X = AB^{-1}\n",
    "$$\n",
    "όπου η σχέση (2) προήλθε από τον τύπο της Frobenius norm (σχέση (132) σελίδα 14)\n",
    "και την σχέση (119) σελίδα 13.\n",
    "<br>\n",
    "Επίσης γνωρίζουμε ότι ο Β αντιστρέφεται αφού είναι τετραγωνικός πίνακας d x d και είναι rank(B) = d ,δηλαδή όλες οι στήλες και οι σειρές του είναι γραμμικά ανεξάρτητες (και άρα και ο ανάστροφος του Β αντιστρέφεται).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x1,x2):\n",
    "    return (x1-2)**2 + (x2-3)**2\n",
    "\n",
    "def f1_derivative(bool,x): #the bool is used at gradient descent to choose which partial derivative we want to use\n",
    "    if bool == 1: #if it is 1 we want the df2/dx1 derivative\n",
    "        return 2*(x-2)\n",
    "    else:\n",
    "        return 2*(x-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x1,x2):\n",
    "    return (4 - x2)**2 + 20*((x1 + 3) - (x2-3)**2)**2\n",
    "\n",
    "def f2_derivative(bool,x1,x2): #the bool is used to determine which partial derivative we want to use\n",
    "    if bool == 1: #if it is 1 we want the df2/dx1 derivative\n",
    "        return 40*((x1+3) - (x2-3)**2)\n",
    "    else:\n",
    "        return -2*(4-x2) + 40*((x1+3) - (x2-3)**2)*(-2*(x2-3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x1_f1 = 0,x2_f1 = 0,x1_f2 = 0,x2_f2 = 0,learning_rate = 0.5,iterations = 10,one_function = False):\n",
    "    F1 = []\n",
    "    F2 = [] \n",
    "    Iter = [] #3 empty lists to return the values we want for the plots\n",
    "    for i in range(iterations):\n",
    "        x1_f1 = x1_f1 - (learning_rate*f1_derivative(1,x1_f1)) #predict new values for x1,x2\n",
    "        x2_f1 = x2_f1 - (learning_rate*f1_derivative(2,x2_f1)) #for the f1 function\n",
    "        F1.append(f1(x1_f1,x2_f1)) #append the function's value to the list\n",
    "        if one_function != True : #if we want also to use gradient descent for f2\n",
    "            x1_f2 = x1_f2 - (learning_rate*f2_derivative(1,x1_f2,x2_f2)) #predict new values for x1,x2\n",
    "            x2_f2 = x2_f2 - (learning_rate*f2_derivative(2,x1_f2,x2_f2)) #for the f2 function\n",
    "            F2.append(f2(x1_f2,x2_f2)) #append the function's value to the list\n",
    "        Iter.append(i) #add the number of iterations\n",
    "    return F1,F2,Iter #return them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Function Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A plot for the default values of gradient descent (learning rate = 0.5 ,x1=x2=0,iterations = 10)\n",
    "F1,F2,iterations = gradient_descent(one_function=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Function Plot $$ $$\n",
    "Learning Rate is set to 0.001 because at 0.5 the algorithm declines causing an overflow error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A plot for the default values of gradient descent (learning rate = 0.001 ,x1=x2=0,iterations = 10)\n",
    "F1,F2,iterations = gradient_descent(learning_rate=0.001)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(F2,iterations,marker='o', color='red')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 with different values for the learning rate ,iterations and x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.01 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.01 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.01 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 0.01 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ανεξάρτητα τις αρχικές τιμές των x1,x2 η συνάρτηση gradient descent συγκλίνει προς το 0.Αυτό θα είναι πιο προφανές στο παρακάτω παράδειγμα που θα έχουμε περισσότερα iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.01 ,medium iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.01 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.01 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 0.01 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ανεξάρτητα των αρχικών τιμών,όλες οι περιπτώσεις μας συγκλίνουν στο 0.Φαίνεται επίσης ότι οι τιμή της gradient για τις καλύτερες επιλογές των x1,x2 (όπως το (0,0)) συγκλίνει πιο γρήγορα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.001 ,medium iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.001 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.001 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 0.001 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μειώνοντας τον παράγοντα learning rate βλέπουμε ότι ακόμα συγκλίνουν όλες οι συναρτήσεις μας στο 0 ,αλλά χρειάζονται παραπάνω επαναλήψεις πάλι."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.001 ,high iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.001 ,iterations=1000,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.001 ,iterations=1000,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 0.001 ,iterations=1000,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως φαίνεται παραπάνω,στις 100 επαναλήψεις δεν φαινόταν κάτι.Όμως στις 1000 επαναλήψεις είναι προφανές ότι για όλες τις τιμές των x1,x2 η συνάρτηση gradient descent συγκλίνει στο 0.\n",
    "Θα δοκιμάσουμε τώρα και μεγάλες τιμές του learning rate για την συνάρτηση f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 1 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 1 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 1 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 1 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε μία απόκλιση τώρα,ότι οι τιμές της συνάρτησης απομακρύνονται από το 0,αντί να το πλησιάζουν"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 1 ,medium iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 1 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 1 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 1 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στις 100 επαναλήψεις είναι προφανές ότι πλέον αντί για σύγκλιση,έχουμε απόκλιση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 5 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 5 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 5 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 5 ,iterations=10,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ενώ όλες οι περιπτώσεις συγκλίνουν στο 0 αρχικά,μετά την 8η επανάληψη αρχίζουν και αποκλίνουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 5,medium iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 5 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='red')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 5 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='purple')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -15,x2_f1 = -15,learning_rate = 5 ,iterations=100,one_function=True)\n",
    "plt.scatter(F1,iterations,marker='o', color='orange')\n",
    "plt.plot(F1,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα συμπεράσματα που μπορούμε να εξάγουμε από τις παραπάνω δοκιμές είναι τα εξής :\n",
    "$$ $$\n",
    "Αρχικά παρατηρούμε ότι ανεξάρτητα τις αρχικές τιμές που δίνουμε στα x1,x2 ,η τιμή της συνάρτησης f1 θα συγκλίνει στο 0.Όμως για 'καλές αρχικές τιμές' ,δηλαδή για τιμές ,για τις οποίες η f1 παίρνει τιμές κοντά στο \n",
    "0,ο αλγόριθμος gradient descent είναι πιο αποδοτικός (συγκλίνει ταχύτερα στο 0).\n",
    "$$ $$\n",
    "Όσο μειώνουμε το learning rate κάτω από το 0.5 ,τόσο πιο αργά συγκλίνει στο 0,επομένως χρειάζεται να κάνουμε παραπάνω επαναλήψεις για να συγκλίνει ο αλγόριθμος.Όμως όταν αυξήσουμε την τιμή του learning rate \n",
    "πάνω από το 0.5 ,αποκλίνει η gradient descent (απομακρύνεται από το 0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F2 with different values for the learning rate ,iterations and x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.01 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.001 ,iterations=10)\n",
    "plt.scatter(F2,iterations,marker='o', color='red')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.001 ,iterations=10)\n",
    "plt.scatter(F2,iterations,marker='o', color='purple')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -40,x2_f1 = -40,learning_rate = 0.001 ,iterations=10)\n",
    "plt.scatter(F2,iterations,marker='o', color='orange')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.01 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.001 ,iterations=1000)\n",
    "plt.scatter(F2,iterations,marker='o', color='red')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 10,x2_f1 = 10,learning_rate = 0.001 ,iterations=1000)\n",
    "plt.scatter(F2,iterations,marker='o', color='purple')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -40,x2_f1 = -40,learning_rate = 0.001 ,iterations=1000)\n",
    "plt.scatter(F2,iterations,marker='o', color='orange')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#Learning Rate 0.01 ,low iterations but with different values of x1,x2 each time\n",
    "F1,F2,iterations = gradient_descent(learning_rate = 0.0001 ,iterations=10000)\n",
    "plt.scatter(F2,iterations,marker='o', color='red')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = 1000,x2_f1 = 1222,learning_rate = 0.0001 ,iterations=10000)\n",
    "plt.scatter(F2,iterations,marker='o', color='purple')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "F1,F2,iterations = gradient_descent(x1_f1 = -40,x2_f1 = -40,learning_rate = 0.0001 ,iterations=10000)\n",
    "plt.scatter(F2,iterations,marker='o', color='orange')\n",
    "plt.plot(F2,iterations,color='blue',markersize=10,linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και στις 3 περιπτώσεις παραπάνω παρατηρούμε ότι ανεξάρτητα τις αρχικές τιμές των x1,x2 η τιμή της f2 συγκλίνει στο 0.Η μόνη διαφορά με τις δοκιμές της f1 είναι ότι για τιμές\n",
    "του learning rate > 0.001 γίνεται overflow error ,όπου οι τιμές της f2 ξεπερνούν τα όρια ενός ακεραίου στην python.Αυτό οφείλεται στο ότι η f2 παίρνει πολύ μεγάλες τιμές ακόμα και για μικρά x1,x2 και επειδή όσο\n",
    "αυξάνουμε το learning rate υπάρχει απόκλιση (όπως είδαμε από την f1) ενάντι σύγκλισης ,οι τιμές τις f2 αυξάνονται αντί να τείνουν στο 0.Ένα παράδειγμα είναι από κάτω :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Value of f2 for x1=x2=10 : ' + str(f2(10,10)))\n",
    "print('Value of f2 for x1=x2=0 : ' + str(f2(0,0)))\n",
    "print('Value of f2 for x1=3,x2=0 : ' + str(f2(3,0)))\n",
    "print('Value of f2 for x1=5,x2=3 : ' + str(f2(5,3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βλέπουμε ότι η f2 παίρνει αρκετά μεγάλες τιμές για μικρές τιμές των x1,x2.\n",
    "$$ $$\n",
    "Σαν την f1 ,και η f2 συγκλίνει στο 0 και όσο μειώνουμε το learning rate τόσο πρέπει να αυξάνουμε τις επαναλήψεις.Όμως σε αντίθεση με την f1,η f2 σε κάθε επανάληψη (ανεξάρτητα της αρχικής τιμής των x1,x2) συγκλίνει\n",
    "στην ίδια τιμή κάθε φορά."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f80286ec14225004d303b9cca7f12330f39a77ffc5ede4ad149bc841ccd55ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
